{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOME = '/srv/home/christinedk/wp_internship/'\n",
    "DATA_DIR = HOME + 'data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/sklearn/feature_extraction/image.py:167: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  dtype=np.int):\n",
      "/usr/lib/python3/dist-packages/sklearn/linear_model/least_angle.py:35: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps,\n",
      "/usr/lib/python3/dist-packages/sklearn/linear_model/least_angle.py:597: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "/usr/lib/python3/dist-packages/sklearn/linear_model/least_angle.py:836: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "/usr/lib/python3/dist-packages/sklearn/linear_model/least_angle.py:862: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, positive=False):\n",
      "/usr/lib/python3/dist-packages/sklearn/linear_model/least_angle.py:1097: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "/usr/lib/python3/dist-packages/sklearn/linear_model/least_angle.py:1344: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "/usr/lib/python3/dist-packages/sklearn/linear_model/least_angle.py:1480: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, positive=False):\n",
      "/usr/lib/python3/dist-packages/sklearn/linear_model/randomized_l1.py:152: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  precompute=False, eps=np.finfo(np.float).eps,\n",
      "/usr/lib/python3/dist-packages/sklearn/linear_model/randomized_l1.py:320: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, random_state=None,\n",
      "/usr/lib/python3/dist-packages/sklearn/linear_model/randomized_l1.py:580: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=4 * np.finfo(np.float).eps, n_jobs=None,\n",
      "/usr/lib/python3/dist-packages/sklearn/decomposition/online_lda.py:31: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  EPS = np.finfo(np.float).eps\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from dateutil import parser\n",
    "import datetime\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pylab as plt\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "import sys\n",
    "sys.path.append('/srv/home/christinedk/wp_internship/collaboration/')\n",
    "from features.article_history import *\n",
    "from features.talk_history import *\n",
    "from utils import read_revisions, np_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/srv/home/christinedk/wp_internship/data\n"
     ]
    }
   ],
   "source": [
    "cd ../data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 3.5G\r\n",
      "-rw-r--r-- 1 christinedk wikidev 1006M Mar 23 13:27 page_history-advert-meta-info.json\r\n",
      "-rw-r--r-- 1 christinedk wikidev  323M Mar 23 13:26 page_history-autobiography-meta-info.json\r\n",
      "-rw-r--r-- 1 christinedk wikidev  240M Mar 23 13:27 page_history-fanpov-meta-info.json\r\n",
      "-rw-r--r-- 1 christinedk wikidev  1.1G Mar 23 13:26 page_history-peacock-meta-info.json\r\n",
      "-rw-r--r-- 1 christinedk wikidev  910M Mar 23 13:25 page_history-weasel-meta-info.json\r\n"
     ]
    }
   ],
   "source": [
    "ls -lh page_history/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for template in ['autobiography','weasel','advert','fanpov','peacock']:\n",
    "    print(template)\n",
    "    df = pd.read_json('/srv/home/christinedk/wp_internship/data/page_history/page_history-{}-meta-info.json'.format(template),\n",
    "                 lines=True)\n",
    "    df = df[df.page_namespace==0]\n",
    "    df = df.sort_values(by=['page_id','revision_id'], ascending=True)\n",
    "    df['has_template'] = df['has_template'].fillna(0).astype(int)\n",
    "    \n",
    "    tag_added = df[(df.has_template.diff()==1)&(~df.revision_is_identity_reverted)][['event_timestamp','event_user_id','page_id','page_title']]\n",
    "    tag_added = tag_added[tag_added.event_user_id!=7328338]\n",
    "    tag_removed = df[(df.has_template.diff()==-1)&(~df.revision_is_identity_reverted)][['event_timestamp','event_user_id','page_id','page_title']]\n",
    "    tag_removed = tag_removed[tag_removed.event_user_id!=7328338]\n",
    "    labels = tag_added[['event_timestamp','page_id']].drop_duplicates()\n",
    "    print('num tags: ',len(labels))\n",
    "\n",
    "    tag_added.to_csv('/srv/home/christinedk/wp_internship/data/tag_events/{}_tag_added.csv'.format(template),\n",
    "                     index=False)\n",
    "    tag_removed.to_csv('/srv/home/christinedk/wp_internship/data/tag_events/{}_tag_removed.csv'.format(template),\n",
    "                     index=False)\n",
    "    labels.to_csv('/srv/home/christinedk/wp_internship/data/labels/{}.csv'.format(template),index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map articles to talk pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "autobiography\n",
      "reading article history\n",
      "3831\n",
      "reading talk history\n",
      "4159\n",
      "number of merged pages: 3810\n",
      "\n",
      "weasel\n",
      "reading article history\n",
      "1382\n",
      "reading talk history\n",
      "8414\n",
      "number of merged pages: 1382\n",
      "\n",
      "advert\n",
      "reading article history\n",
      "7235\n",
      "reading talk history\n",
      "44603\n",
      "number of merged pages: 7167\n",
      "\n",
      "fanpov\n",
      "reading article history\n",
      "828\n",
      "reading talk history\n",
      "1896\n",
      "number of merged pages: 806\n",
      "\n",
      "peacock\n",
      "reading article history\n",
      "5104\n",
      "reading talk history\n",
      "12943\n",
      "number of merged pages: 5066\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for template in ['autobiography','weasel','advert','fanpov','peacock']:\n",
    "    print(template)\n",
    "    \n",
    "    print('reading article history')\n",
    "    article = pd.read_json('/srv/home/christinedk/wp_internship/data/page_history/page_history-{}-meta-info.json'.format(template),\n",
    "                 lines=True)\n",
    "    article = article[article.page_namespace==0]\n",
    "    print(article.page_id.nunique())\n",
    "\n",
    "    print('reading talk history')\n",
    "    talk = pd.read_json('/srv/home/christinedk/wp_internship/data/page_history/talk-activity-{}-meta-info.json'.format(template),\n",
    "                 lines=True)\n",
    "    #extract_from_json('/srv/home/christinedk/wp_internship/data/talk_history/talk-activity-{}-meta-info.json'.format(template))\n",
    "    print(talk.page_id.nunique())\n",
    "\n",
    "    article['page_title'] = article.page_title.str.replace('_',' ').astype(str)\n",
    "    talk['page_title'] = talk.page_title.str[5:]\n",
    "\n",
    "    talk = talk[['page_id','page_title']].drop_duplicates()\n",
    "    article = article[['page_id','page_title']].drop_duplicates()\n",
    "\n",
    "    merged = talk.merge(article,on='page_title')\n",
    "    merged.columns = ['talk_page_id','page_title','article_page_id']\n",
    "    print('number of merged pages: {}'.format(len(merged)))\n",
    "    print()\n",
    "\n",
    "    merged.to_csv('/srv/home/christinedk/wp_internship/data/article_talk_mappings/{}.csv'.format(template),index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "autobiography\n",
      "weasel\n",
      "advert\n",
      "fanpov\n",
      "peacock\n"
     ]
    }
   ],
   "source": [
    "for template in ['autobiography','weasel','advert','fanpov','peacock']:\n",
    "    print(template)\n",
    "\n",
    "    labels = pd.read_csv('/srv/home/christinedk/wp_internship/data/labels/{}.csv'.format(template))\n",
    "\n",
    "    df = pd.read_json('/srv/home/christinedk/wp_internship/data/page_history/page_history-{}-meta-info.json'.format(template),\n",
    "                 lines=True)\n",
    "    df = df[df.page_namespace==0]\n",
    "    df['has_template'] = df['has_template'].fillna(0).astype(int)\n",
    "\n",
    "    no_tag = df[(df.has_template.diff()==0)&\n",
    "                (~df.revision_is_identity_reverted)][['event_timestamp','event_user_id','page_id','page_title','revision_id']]\n",
    "\n",
    "    labels_per_page = labels.groupby('page_id').count().reset_index()\n",
    "    labels_per_page = {key:value for key, value in labels_per_page.values}\n",
    "    negatives = no_tag.groupby('page_id').apply(lambda x: x.sample(n=min(labels_per_page.get(x.name,0)*5,len(x))))\n",
    "    negatives = negatives.reset_index(drop=True)[['event_timestamp','page_id']]\n",
    "    print(len(negatives))\n",
    "\n",
    "    negatives.to_csv('/srv/home/christinedk/wp_internship/data/negative_labels/{}.csv'.format(template),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subsample talk (negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "autobiography\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 54/21116 [00:00<00:39, 534.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of labels:  21116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21116/21116 [00:28<00:00, 731.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scanning talk page\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "1120\n",
      "weasel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/6610 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of labels:  6610\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6610/6610 [00:10<00:00, 640.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scanning talk page\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "210000\n",
      "220000\n",
      "230000\n",
      "240000\n",
      "250000\n",
      "260000\n",
      "270000\n",
      "280000\n",
      "290000\n",
      "300000\n",
      "310000\n",
      "320000\n",
      "330000\n",
      "340000\n",
      "350000\n",
      "360000\n",
      "370000\n",
      "380000\n",
      "390000\n",
      "400000\n",
      "410000\n",
      "420000\n",
      "430000\n",
      "440000\n",
      "450000\n",
      "460000\n",
      "470000\n",
      "480000\n",
      "490000\n",
      "500000\n",
      "510000\n",
      "520000\n",
      "530000\n",
      "540000\n",
      "550000\n",
      "560000\n",
      "570000\n",
      "580000\n",
      "590000\n",
      "600000\n",
      "610000\n",
      "620000\n",
      "630000\n",
      "640000\n",
      "650000\n",
      "660000\n",
      "670000\n",
      "680000\n",
      "690000\n",
      "700000\n",
      "710000\n",
      "720000\n",
      "730000\n",
      "740000\n",
      "750000\n",
      "760000\n",
      "770000\n",
      "780000\n",
      "790000\n",
      "800000\n",
      "810000\n",
      "820000\n",
      "830000\n",
      "840000\n",
      "850000\n",
      "860000\n",
      "870000\n",
      "880000\n",
      "890000\n",
      "900000\n",
      "910000\n",
      "920000\n",
      "930000\n",
      "940000\n",
      "950000\n",
      "960000\n",
      "970000\n",
      "980000\n",
      "990000\n",
      "1000000\n",
      "1010000\n",
      "1020000\n",
      "1030000\n",
      "1040000\n",
      "1050000\n",
      "1060000\n",
      "1070000\n",
      "1080000\n",
      "1090000\n",
      "1100000\n",
      "1110000\n",
      "1120000\n",
      "1130000\n",
      "1140000\n",
      "1150000\n",
      "1160000\n",
      "1170000\n",
      "1180000\n",
      "1190000\n",
      "1200000\n",
      "1210000\n",
      "1220000\n",
      "1230000\n",
      "1240000\n",
      "1250000\n",
      "1260000\n",
      "1270000\n",
      "1280000\n",
      "1290000\n",
      "1300000\n",
      "1310000\n",
      "1320000\n",
      "1330000\n",
      "1340000\n",
      "1350000\n",
      "1360000\n",
      "1370000\n",
      "1380000\n",
      "1390000\n",
      "1400000\n",
      "1410000\n",
      "1420000\n",
      "1430000\n",
      "1440000\n",
      "1450000\n",
      "1460000\n",
      "1470000\n",
      "1480000\n",
      "1490000\n",
      "1500000\n",
      "1510000\n",
      "1520000\n",
      "1530000\n",
      "1540000\n",
      "1550000\n",
      "1560000\n",
      "1570000\n",
      "1580000\n",
      "1590000\n",
      "1600000\n",
      "1610000\n",
      "1620000\n",
      "1630000\n",
      "1640000\n",
      "1650000\n",
      "1660000\n",
      "1670000\n",
      "1680000\n",
      "1690000\n",
      "1700000\n",
      "1710000\n",
      "1720000\n",
      "1730000\n",
      "1740000\n",
      "1750000\n",
      "1760000\n",
      "1770000\n",
      "1780000\n",
      "1790000\n",
      "1800000\n",
      "1810000\n",
      "1820000\n",
      "1830000\n",
      "1840000\n",
      "1850000\n",
      "1860000\n",
      "1870000\n",
      "1880000\n",
      "1890000\n",
      "1900000\n",
      "1910000\n",
      "1920000\n",
      "1930000\n",
      "1940000\n",
      "1950000\n",
      "1960000\n",
      "1970000\n",
      "1980000\n",
      "1990000\n",
      "2000000\n",
      "2010000\n",
      "2020000\n",
      "2030000\n",
      "2040000\n",
      "2050000\n",
      "2060000\n",
      "2070000\n",
      "2080000\n",
      "2090000\n",
      "2100000\n",
      "2110000\n",
      "2120000\n",
      "2130000\n",
      "2140000\n",
      "2150000\n",
      "2160000\n",
      "2170000\n",
      "2180000\n",
      "2190000\n",
      "2200000\n",
      "2210000\n",
      "2220000\n",
      "2230000\n",
      "2240000\n",
      "2250000\n",
      "2260000\n",
      "2270000\n",
      "2280000\n",
      "2290000\n",
      "2300000\n",
      "2310000\n",
      "2320000\n",
      "2330000\n",
      "2340000\n",
      "2350000\n",
      "2360000\n",
      "2370000\n",
      "2380000\n",
      "2390000\n",
      "2400000\n",
      "2410000\n",
      "2420000\n",
      "2430000\n",
      "2440000\n",
      "2450000\n",
      "2460000\n",
      "2470000\n",
      "2480000\n",
      "2490000\n",
      "2500000\n",
      "2510000\n",
      "2520000\n",
      "2530000\n",
      "2540000\n",
      "2550000\n",
      "2560000\n",
      "2570000\n",
      "2580000\n",
      "2590000\n",
      "2600000\n",
      "2610000\n",
      "2620000\n",
      "2630000\n",
      "2640000\n",
      "2650000\n",
      "2660000\n",
      "2670000\n",
      "2680000\n",
      "2690000\n",
      "2700000\n",
      "2710000\n",
      "2720000\n",
      "2730000\n",
      "2740000\n",
      "2750000\n",
      "2760000\n",
      "2770000\n",
      "2780000\n",
      "2790000\n",
      "2800000\n",
      "2810000\n",
      "2820000\n",
      "2830000\n",
      "2840000\n",
      "2850000\n",
      "2860000\n",
      "2870000\n",
      "2880000\n",
      "2890000\n",
      "2900000\n",
      "1120\n",
      "advert\n",
      "number of labels:  37843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37843/37843 [00:53<00:00, 706.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scanning talk page\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "210000\n",
      "220000\n",
      "230000\n",
      "240000\n",
      "250000\n",
      "260000\n",
      "270000\n",
      "280000\n",
      "290000\n",
      "300000\n",
      "310000\n",
      "320000\n",
      "330000\n",
      "340000\n",
      "350000\n",
      "360000\n",
      "370000\n",
      "380000\n",
      "390000\n",
      "400000\n",
      "410000\n",
      "420000\n",
      "430000\n",
      "440000\n",
      "450000\n",
      "460000\n",
      "470000\n",
      "480000\n",
      "490000\n",
      "500000\n",
      "510000\n",
      "520000\n",
      "530000\n",
      "540000\n",
      "550000\n",
      "560000\n",
      "570000\n",
      "580000\n",
      "590000\n",
      "600000\n",
      "610000\n",
      "620000\n",
      "630000\n",
      "640000\n",
      "650000\n",
      "660000\n",
      "670000\n",
      "680000\n",
      "690000\n",
      "700000\n",
      "710000\n",
      "720000\n",
      "730000\n",
      "740000\n",
      "750000\n",
      "760000\n",
      "770000\n",
      "780000\n",
      "790000\n",
      "800000\n",
      "810000\n",
      "820000\n",
      "830000\n",
      "840000\n",
      "850000\n",
      "860000\n",
      "870000\n",
      "880000\n",
      "890000\n",
      "900000\n",
      "910000\n",
      "920000\n",
      "930000\n",
      "940000\n",
      "950000\n",
      "960000\n",
      "970000\n",
      "980000\n",
      "990000\n",
      "1000000\n",
      "1010000\n",
      "1020000\n",
      "1030000\n",
      "1040000\n",
      "1050000\n",
      "1060000\n",
      "1070000\n",
      "1080000\n",
      "1090000\n",
      "1100000\n",
      "1110000\n",
      "1120000\n",
      "1130000\n",
      "1140000\n",
      "1150000\n",
      "1160000\n",
      "1170000\n",
      "1180000\n",
      "1190000\n",
      "1200000\n",
      "1210000\n",
      "1220000\n",
      "1230000\n",
      "1240000\n",
      "1250000\n",
      "1260000\n",
      "1270000\n",
      "1280000\n",
      "1290000\n",
      "1300000\n",
      "1310000\n",
      "1320000\n",
      "1330000\n",
      "1340000\n",
      "1350000\n",
      "1120\n",
      "fanpov\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 68/3605 [00:00<00:05, 671.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of labels:  3605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3605/3605 [00:05<00:00, 682.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scanning talk page\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "1120\n",
      "peacock\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 25/25863 [00:00<01:43, 249.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of labels:  25863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25863/25863 [00:36<00:00, 714.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scanning talk page\n",
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "210000\n",
      "220000\n",
      "230000\n",
      "240000\n",
      "250000\n",
      "260000\n",
      "270000\n",
      "280000\n",
      "290000\n",
      "300000\n",
      "310000\n",
      "320000\n",
      "330000\n",
      "340000\n",
      "350000\n",
      "360000\n",
      "370000\n",
      "380000\n",
      "390000\n",
      "400000\n",
      "410000\n",
      "420000\n",
      "430000\n",
      "440000\n",
      "450000\n",
      "460000\n",
      "470000\n",
      "480000\n",
      "490000\n",
      "500000\n",
      "510000\n",
      "520000\n",
      "530000\n",
      "540000\n",
      "550000\n",
      "560000\n",
      "570000\n",
      "580000\n",
      "590000\n",
      "600000\n",
      "610000\n",
      "620000\n",
      "630000\n",
      "640000\n",
      "650000\n",
      "660000\n",
      "670000\n",
      "680000\n",
      "690000\n",
      "700000\n",
      "710000\n",
      "720000\n",
      "730000\n",
      "740000\n",
      "750000\n",
      "760000\n",
      "770000\n",
      "780000\n",
      "790000\n",
      "800000\n",
      "810000\n",
      "820000\n",
      "830000\n",
      "1120\n"
     ]
    }
   ],
   "source": [
    "for template in ['autobiography','weasel','advert','fanpov','peacock']:\n",
    "    print(template)\n",
    "    talk_revisions = read_revisions(DATA_DIR+'talk_history/talk-activity-{}-meta-info.json'.format(template),\n",
    "                                    rename=True).sort_values(by='event_timestamp')\n",
    "    \n",
    "    article_talk_mappings = pd.read_csv(DATA_DIR+'article_talk_mappings/{}.csv'.format(template),\n",
    "                                   usecols=['talk_page_id','article_page_id'])\n",
    "    labels = pd.read_csv(DATA_DIR+'negative_labels/{}.csv'.format(template),parse_dates=['event_timestamp'])\n",
    "    labels = labels.join(article_talk_mappings.set_index('article_page_id')[['talk_page_id']],on='page_id').dropna()\n",
    "    print('number of labels: ',len(labels))\n",
    "\n",
    "    talk_pages = talk_revisions.groupby('page_id')\n",
    "    last_revision = []\n",
    "\n",
    "    for tag_date, page_id, talk_page_id in tqdm(labels.values):\n",
    "        tag_talk_revisions = talk_pages.get_group(talk_page_id)\n",
    "        tag_talk_revisions = tag_talk_revisions[tag_talk_revisions.event_timestamp.dt.date <= tag_date]\n",
    "        if len(tag_talk_revisions) > 0:\n",
    "            edit = tag_talk_revisions.iloc[-1].to_dict()\n",
    "            edit['tag_date'] = tag_date\n",
    "            edit['talk_page_id'] = talk_page_id\n",
    "            last_revision.append(edit)\n",
    "    last_revision = pd.DataFrame(last_revision).groupby('talk_page_id')\n",
    "                \n",
    "    last_revision_text = []\n",
    "    print('scanning talk page')\n",
    "    with open(DATA_DIR + 'talk_history/talk-text-{}-meta-info.json'.format(template),'rb') as f:\n",
    "        for i,line in enumerate(f):\n",
    "            if i%10000 == 0:\n",
    "                print(i)\n",
    "            snapshot=json.loads(line)\n",
    "            if snapshot['page_id'] in last_revision.groups.keys():\n",
    "                if snapshot['revision_id'] in last_revision.get_group(snapshot['page_id']).revision_id.values:\n",
    "                    last_revision_text.append(snapshot)\n",
    "    print(len(talk))\n",
    "    \n",
    "    with open(DATA_DIR+'talk_history/talk-subset-negative-{}.json'.format(template),'w') as f:\n",
    "        json.dump(talk_dump, f, default=np_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "HOME = '/srv/home/christinedk/wp_internship/'\n",
    "DATA_DIR = HOME + 'data/'\n",
    "\n",
    "from collections import defaultdict\n",
    "from dateutil import parser\n",
    "\n",
    "def np_encoder(object):\n",
    "    if isinstance(object, np.generic):\n",
    "        return object.item()\n",
    "    \n",
    "for template in ['autobiography','weasel','advert','fanpov','peacock']:\n",
    "    print(template)\n",
    "\n",
    "    article_talk_mappings = pd.read_csv(DATA_DIR+'article_talk_mappings/{}.csv'.format(template),\n",
    "                                       usecols=['talk_page_id','article_page_id'])\n",
    "    labels = pd.read_csv(DATA_DIR+'negative_labels/{}.csv'.format(template),parse_dates=['event_timestamp'])\n",
    "    labels = labels.join(article_talk_mappings.set_index('article_page_id')[['talk_page_id']],on='page_id').dropna()\n",
    "    print('number of labels: ',len(labels))\n",
    "    \n",
    "    page_labels = labels.groupby('talk_page_id')\n",
    "    pages = labels.talk_page_id.unique()\n",
    "    \n",
    "    talk = {}\n",
    "    with open(DATA_DIR + 'talk_history/talk-text-{}-meta-info.json'.format(template),'rb') as f:\n",
    "        for line in f:\n",
    "            snapshot=json.loads(line)\n",
    "            page_id = snapshot['page_id']\n",
    "            if page_id not in pages:\n",
    "                continue\n",
    "\n",
    "            snapshot_date = parser.parse(snapshot['revision_timestamp']).replace(tzinfo=None)\n",
    "            page_lable_dates = page_labels.get_group(snapshot['page_id'])['event_timestamp']\n",
    "            date_diffs = (snapshot_date - page_lable_dates).dt.days\n",
    "            min_ind = date_diffs.idxmin()\n",
    "            min_diff = date_diffs[min_ind]\n",
    "            if 0 <= min_diff < talk.get((page_id,page_lable_dates[min_ind])[0],365):\n",
    "                talk[page_id,page_lable_dates[min_ind]]=(min_diff,snapshot)\n",
    "\n",
    "    print(len(talk))\n",
    "    talk_dump = [{'talk_page_id':key[0],'event_timestamp':str(key[1]),**value[1]} for key, value in talk.items()]\n",
    "    with open(DATA_DIR+'talk_history/talk-subset-negative-{}.json'.format(template),'w') as f:\n",
    "        json.dump(talk_dump, f, default=np_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subsample talk (positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "HOME = '/srv/home/christinedk/wp_internship/'\n",
    "DATA_DIR = HOME + 'data/'\n",
    "\n",
    "from collections import defaultdict\n",
    "from dateutil import parser\n",
    "\n",
    "def np_encoder(object):\n",
    "    if isinstance(object, np.generic):\n",
    "        return object.item()\n",
    "    \n",
    "for template in ['autobiography','weasel','advert','fanpov','peacock']:\n",
    "    print(template)\n",
    "\n",
    "    article_talk_mappings = pd.read_csv(DATA_DIR+'article_talk_mappings/{}.csv'.format(template),\n",
    "                                       usecols=['talk_page_id','article_page_id'])\n",
    "    labels = pd.read_csv(DATA_DIR+'labels/{}.csv'.format(template),parse_dates=['event_timestamp'])\n",
    "    labels = labels.join(article_talk_mappings.set_index('article_page_id')[['talk_page_id']],on='page_id').dropna()\n",
    "    print('number of labels: ',len(labels))\n",
    "    \n",
    "    page_labels = labels.groupby('talk_page_id')\n",
    "    pages = labels.talk_page_id.unique()\n",
    "    \n",
    "    talk = {}\n",
    "    with open(DATA_DIR + 'talk_history/talk-text-{}-meta-info.json'.format(template),'rb') as f:\n",
    "        for line in f:\n",
    "            snapshot=json.loads(line)\n",
    "            page_id = snapshot['page_id']\n",
    "            if page_id not in pages:\n",
    "                continue\n",
    "\n",
    "            snapshot_date = parser.parse(snapshot['revision_timestamp']).replace(tzinfo=None)\n",
    "            page_lable_dates = page_labels.get_group(snapshot['page_id'])['event_timestamp']\n",
    "            date_diffs = (snapshot_date - page_lable_dates).dt.days\n",
    "            min_ind = date_diffs.idxmin()\n",
    "            min_diff = date_diffs[min_ind]\n",
    "            if 0 <= min_diff < talk.get((page_id,page_lable_dates[min_ind])[0],365):\n",
    "                talk[page_id,page_lable_dates[min_ind]]=(min_diff,snapshot)\n",
    "\n",
    "    print(len(talk))\n",
    "    talk_dump = [{'talk_page_id':key[0],'event_timestamp':str(key[1]),**value[1]} for key, value in talk.items()]\n",
    "    with open(DATA_DIR+'talk_history/talk-subset-{}.json'.format(template),'w') as f:\n",
    "        json.dump(talk_dump, f, default=np_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
