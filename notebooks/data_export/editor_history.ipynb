{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "HOME='/srv/home/christinedk/wp_internship/'\n",
    "sys.path.append(HOME + 'collaboration/')\n",
    "from config import TEMPLATES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time  import time\n",
    "import matplotlib.pylab as plt\n",
    "from pyspark.sql.functions import udf, col, explode, regexp_replace, first, mean, size, count, collect_list\n",
    "import dateutil \n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from pyspark.sql.functions import to_timestamp, min, max\n",
    "\n",
    "from data_export import getTemplatesRegexRelaibility, getTemplatesRegex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: `page_history': File exists\r\n"
     ]
    }
   ],
   "source": [
    "TEMPLATES = ['weasel','peacock','autobiography','advert','fanpov']\n",
    "outputHDFS = 'page_history'\n",
    "!hadoop fs -mkdir $outputHDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read back revisions with template\n",
    "revisions_with_template = spark.read.parquet('page_history/templates.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Select subset of mediawiki history containing all page titles with revisions\n",
    "\n",
    "pages_templates_subset = revisions_with_template.select('page_title').distinct()\n",
    "pages_templates_subset.createOrReplaceTempView('pages_templates_subset')\n",
    "\n",
    "users =  spark.sql('''\n",
    "        SELECT DISTINCT w.event_user_id\n",
    "        FROM wmf.mediawiki_history w\n",
    "        WHERE w.snapshot =\"2020-09\" and w.wiki_db =\"enwiki\" AND  \n",
    "        w.event_entity = 'revision' AND w.page_title IN (\n",
    "        SELECT page_title FROM pages_templates_subset)  \n",
    "        ''')\n",
    "users.cache()\n",
    "users = users.filter(users['event_user_id']!=7328338) # bad user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "405869"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#revisions_with_template.where(revisions_with_template['col']==template)\n",
    "\n",
    "users = users.where(users['event_user_id'==])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "users.createOrReplaceTempView('tmp_users')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting user histories\n"
     ]
    }
   ],
   "source": [
    "print('extracting user histories')\n",
    "t1 = time()\n",
    "user_histories =  spark.sql('''\n",
    "SELECT w.event_user_text, w.event_timestamp, w.page_title,w.page_id,w.page_namespace,\n",
    "w.revision_id, w.revision_is_identity_reverted, revision_is_identity_revert,\n",
    "w.revision_minor_edit, w.revision_text_bytes, \n",
    "w.revision_first_identity_reverting_revision_id,\n",
    "w.event_user_id,w.event_user_registration_timestamp, \n",
    "SIZE(w.event_user_groups) as num_groups,\n",
    "SIZE(w.event_user_blocks_historical) as num_blocks_historical, \n",
    "SIZE(w.event_user_blocks) as num_curr_blocks\n",
    "FROM wmf.mediawiki_history w\n",
    "\n",
    "WHERE w.snapshot =\"2020-09\" and w.wiki_db =\"enwiki\" AND  \n",
    "w.event_entity = 'revision' \n",
    "AND w.event_user_id IN (SELECT event_user_id FROM tmp_users) \n",
    "AND SIZE(w.event_user_is_bot_by) = 0\n",
    "ORDER BY w.event_user_id\n",
    "''') \n",
    "#user_histories.cache()\n",
    "user_histories.write.parquet('editors/user_histories.parquet',mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark - YARN",
   "language": "python",
   "name": "spark_yarn_pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
