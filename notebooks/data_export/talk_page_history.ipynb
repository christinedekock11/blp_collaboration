{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/christinedk/wp_internship/collaboration/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time  import time\n",
    "from data_export import getTemplatesRegexRelaibility, getTemplatesRegex\n",
    "from pyspark.sql.functions import udf, col, explode, regexp_replace, concat, lit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: `talk': File exists\r\n"
     ]
    }
   ],
   "source": [
    "TEMPLATES = ['weasel','peacock','autobiography','advert','fanpov']\n",
    "outputHDFS = 'talk'\n",
    "!hadoop fs -mkdir $outputHDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find revisions with templates (generated by revision_history notebook)\n",
    "\n",
    "revisions_with_template = spark.read.parquet('tmp/templates.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Format page titles so that they will be recognised in the WikiText database\n",
    "\n",
    "revisions_with_template = revisions_with_template.withColumn('page_title', regexp_replace('page_title', '_', ' '))\n",
    "revisions_with_template = revisions_with_template.withColumn('page_title_talk',\n",
    "                                                         concat(lit(\"Talk:\"),col(\"page_title\")))\n",
    "\n",
    "pages_templates_subset = revisions_with_template.select('page_title_talk').distinct()\n",
    "pages_templates_subset.createOrReplaceTempView('pages_templates_subset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Select subset of WikiText history for Talk pages of pages with templates\n",
    "\n",
    "mediawiki_history_subset =  spark.sql('''\n",
    "        SELECT w.revision_text, w.user_id, w.page_id, w.page_title, w.revision_id, w.revision_timestamp, w.user_text,\n",
    "        w.revision_minor_edit,revision_text_bytes, w.revision_parent_id\n",
    "        FROM wmf.mediawiki_wikitext_history w\n",
    "        WHERE w.snapshot =\"2021-01\" and w.wiki_db =\"enwiki\" AND w.page_namespace=1\n",
    "        AND w.page_title IN (SELECT page_title_talk FROM pages_templates_subset)                   \n",
    "        ''')\n",
    "#mediawiki_history_subset.cache()\n",
    "mediawiki_history_subset.createOrReplaceTempView('mediawiki_history_subset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weasel\n",
      "read table, done 0.10941886901855469\n",
      "save table, done 754.5110597610474\n",
      "21/03/26 13:34:07 INFO compress.CodecPool: Got brand-new decompressor [.snappy]\n",
      "21/03/26 14:03:46 INFO compress.CodecPool: Got brand-new decompressor [.snappy]\n",
      "----- 1794.709993839264\n",
      "peacock\n",
      "read table, done 0.0868842601776123\n",
      "save table, done 99.95339035987854\n",
      "21/03/26 14:05:43 INFO compress.CodecPool: Got brand-new decompressor [.snappy]\n",
      "21/03/26 14:10:27 INFO compress.CodecPool: Got brand-new decompressor [.snappy]\n",
      "----- 291.4793632030487\n",
      "autobiography\n",
      "read table, done 0.09740376472473145\n",
      "save table, done 2829.065163373947\n",
      "21/03/26 14:57:42 INFO compress.CodecPool: Got brand-new decompressor [.snappy]\n",
      "21/03/26 14:57:59 INFO compress.CodecPool: Got brand-new decompressor [.snappy]\n",
      "----- 22.26380729675293\n",
      "advert\n",
      "read table, done 0.10959792137145996\n",
      "save table, done 3170.939197540283\n",
      "21/03/26 15:50:56 INFO compress.CodecPool: Got brand-new decompressor [.snappy]\n",
      "21/03/26 15:56:08 INFO compress.CodecPool: Got brand-new decompressor [.snappy]\n",
      "----- 319.8312587738037\n",
      "fanpov\n",
      "read table, done 0.1250905990600586\n",
      "save table, done 3735.4538736343384\n",
      "21/03/26 16:58:31 INFO compress.CodecPool: Got brand-new decompressor [.snappy]\n",
      "21/03/26 16:58:52 INFO compress.CodecPool: Got brand-new decompressor [.snappy]\n",
      "----- 26.365535497665405\n"
     ]
    }
   ],
   "source": [
    "for template in TEMPLATES:\n",
    "    try:\n",
    "        t1 = time()\n",
    "        print(template)\n",
    "        df = revisions_with_template.where(revisions_with_template['col']==template) # revisions for specific template\n",
    "        df.cache()\n",
    "        t2 = time()\n",
    "        print('read table, done',t2-t1)\n",
    "        t1 = time()        \n",
    "        page_ids = df.select('page_title_talk').distinct() # titles of revisions with specific template\n",
    "        page_ids.createOrReplaceTempView('tmp_page_ids')\n",
    "        reverts = spark.sql('''\n",
    "        SELECT w.revision_text, w.user_id, w.page_id, w.page_title, w.revision_text_bytes, w.revision_id, w.revision_timestamp, w.revision_minor_edit, w.revision_parent_id, w.user_text\n",
    "        FROM mediawiki_history_subset w\n",
    "        WHERE w.page_title IN (SELECT page_title_talk FROM tmp_page_ids) \n",
    "        SORT BY page_id\n",
    "        ''') \n",
    "        reverts.cache()\n",
    "        reverts.write.format('json').save('talk-text/'+template,mode='overwrite')\n",
    "        reverts.drop('revision_text').write.format('json').save('talk-activity/'+template,mode='overwrite')\n",
    "        t2 = time()\n",
    "        print('save table, done',t2-t1)\n",
    "        t1 = time()   \n",
    "        templateout = template.replace(' ','_')\n",
    "        !hadoop fs -text \"talk-text/$template/*\" > $outputHDFS-text-$template-meta-info.json\n",
    "        !hadoop fs -text \"talk-activity/$template/*\" > $outputHDFS-activity-$template-meta-info.json\n",
    "        t2 = time()\n",
    "        print('-----',t2-t1)\n",
    "    except Exception as e:\n",
    "        print('error',e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access '/srv/home/christinedk/wp_internship/notebooks/talk*': No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "ls -lh /srv/home/christinedk/wp_internship/notebooks/talk*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mv talk* /srv/home/christinedk/wp_internship/data/talk_history/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark - YARN",
   "language": "python",
   "name": "spark_yarn_pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
