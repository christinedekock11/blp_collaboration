{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch as th\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConstructor(object):\n",
    "    def __init__(self):\n",
    "        self.num_editor_features = 10\n",
    "        self.num_page_features = 15\n",
    "        self.num_editor_page_features = 8\n",
    "        self.num_collab_dir_feat = 2\n",
    "        self.num_collab_undir_feat = 2\n",
    "\n",
    "    def construct_graph(self, sample):\n",
    "        self.editor_nodes = [d['event_user_id'] for d in sample['user_article']]\n",
    "        self.editor_to_ind = {j:i for i,j in enumerate(self.editor_nodes)}\n",
    "        self.ind_to_editor = {i:j for j,i in self.editor_to_ind.items()}\n",
    "        self.editor_page_links = [(self.editor_to_ind[i],0) for i in self.editor_nodes]\n",
    "        self.collab_links_directed = [(self.editor_to_ind[pair['event_user_id']],self.editor_to_ind[pair['event_user_id_r']]) \n",
    "                                     for pair in sample['collaboration']['directed']]\n",
    "        self.collab_links_undirected = [(self.editor_to_ind[d['pair'][0]],self.editor_to_ind[d['pair'][1]]) \n",
    "                                    for d in sample['collaboration']['undirected']]\n",
    "        \n",
    "        g = dgl.heterograph({\n",
    "             ('editor', 'edits', 'page'): self.editor_page_links,\n",
    "             ('editor', 'collab-dir', 'editor'): self.collab_links_directed,\n",
    "             ('editor', 'collab-undir', 'editor'): self.collab_links_undirected + \\\n",
    "                            [(j,i) for i,j in self.collab_links_undirected]})\n",
    "        return g\n",
    "    \n",
    "    def format_page_features(self,sample):\n",
    "        page_features = [list(sample['article'].values())]\n",
    "        page_features = th.tensor([[i if not np.isnan(i) else 0 for i in sample_feat]\n",
    "                                    for sample_feat in page_features])\n",
    "        return page_features\n",
    "\n",
    "    def format_editor_features(self, sample):\n",
    "        editor_features_lookup = pd.DataFrame(sample['editor'])\\\n",
    "                                            .set_index('event_user_id').to_dict('index')\n",
    "        null_dict = {i: 0 for i in range(self.num_editor_features)}\n",
    "        editor_features = [list(editor_features_lookup.get(i,null_dict).values()) \n",
    "                                     for i in self.editor_nodes]\n",
    "        editor_features = th.tensor([[i if not np.isnan(i) else 0 for i in sample_feat]\n",
    "                                    for sample_feat in editor_features])\n",
    "        return editor_features\n",
    "    \n",
    "    def format_node_features(self,sample):\n",
    "        page_feat = self.format_page_features(sample)\n",
    "        editor_feat = self.format_editor_features(sample)\n",
    "        return {'page':page_feat,'editor':editor_feat}\n",
    "    \n",
    "    def format_edge_features(self,sample):\n",
    "        features = {}\n",
    "        # directed\n",
    "        if len(sample['collaboration']['directed']) > 0:\n",
    "            collab_dir_lookup = pd.DataFrame(sample['collaboration']['directed'])\\\n",
    "                                    .set_index(['event_user_id','event_user_id_r']).to_dict('index')\n",
    "            null_dict = {i: 0 for i in range(self.num_collab_dir_feat)}\n",
    "            ft = [list(collab_dir_lookup.get((self.ind_to_editor[i],self.ind_to_editor[j]),null_dict).values()) \n",
    "                         for i,j in self.collab_links_directed]\n",
    "            features['collab-dir'] = th.tensor([[i if not np.isnan(i) else 0 for i in sample_feat]\n",
    "                                    for sample_feat in ft])\n",
    "\n",
    "            collab_undir_lookup = pd.DataFrame(sample['collaboration']['undirected'])\n",
    "            collab_undir_lookup['id_1'],collab_undir_lookup['id_2'] = zip(*list(collab_undir_lookup['pair']))\n",
    "            collab_undir_lookup = collab_undir_lookup.set_index(['id_1','id_2']).drop('pair',axis=1).to_dict('index')\n",
    "            null_dict = {i: 0 for i in range(self.num_collab_undir_feat)}\n",
    "            collab_undir_features = [list(\n",
    "                        collab_undir_lookup.get((self.ind_to_editor[i],self.ind_to_editor[j]),null_dict).values()) \n",
    "                                     for i,j in self.collab_links_undirected]\n",
    "            ft = collab_undir_features + collab_undir_features\n",
    "            features['collab-undir'] = th.tensor([[i if not np.isnan(i) else 0 for i in sample_feat]\n",
    "                                    for sample_feat in ft])\n",
    "            \n",
    "        # edits\n",
    "        editor_article_features_lookup = pd.DataFrame(sample['user_article'])\\\n",
    "                                                .set_index('event_user_id').to_dict('index')\n",
    "        null_dict = {i: 0 for i in range(self.num_editor_page_features)}\n",
    "        ft = [list(editor_article_features_lookup.get(i,null_dict).values()) \n",
    "                                             for i in self.editor_nodes]\n",
    "        features['edits'] = th.tensor([[i if not np.isnan(i) else 0 for i in sample_feat]\n",
    "                                    for sample_feat in ft])\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def make_graph(self, sample):\n",
    "        graph = self.construct_graph(sample)        \n",
    "        graph.ndata['features'] = self.format_node_features(sample)\n",
    "        graph.edata['features'] = self.format_edge_features(sample)\n",
    "        return graph\n",
    "\n",
    "    #def format_features(graph):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/srv/home/christinedk/wp_internship/features/activity_fanpov.json','rb') as f:\n",
    "    page_history = json.load(f)\n",
    "with open('/srv/home/christinedk/wp_internship/features/editorsfanpov_v2.json','rb') as f:\n",
    "    editor_history = json.load(f)\n",
    "    \n",
    "pos_samples = [{**e,**p} for e,p in zip(page_history, editor_history)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = []\n",
    "for sample in pos_samples:\n",
    "    graphmaker = GraphConstructor()\n",
    "    g = graphmaker.make_graph(sample)\n",
    "    pos.append((g,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/srv/home/christinedk/wp_internship/negative_features/activity_fanpov.json','rb') as f:\n",
    "    page_history = json.load(f) \n",
    "with open('/srv/home/christinedk/wp_internship/negative_features/editorsfanpov_v2.json','rb') as f:\n",
    "    editor_history = json.load(f)\n",
    "    \n",
    "neg_samples = [{**e,**p} for e,p in zip(page_history, editor_history)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg = []\n",
    "for sample in neg_samples:\n",
    "    graphmaker = GraphConstructor()\n",
    "    g = graphmaker.make_graph(sample)\n",
    "    neg.append((g,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pos + neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.dataloading import GraphDataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "num_examples = len(dataset)\n",
    "num_test = int(num_examples * 0.2)\n",
    "\n",
    "test_sampler = SubsetRandomSampler(torch.arange(num_test))\n",
    "train_sampler = SubsetRandomSampler(torch.arange(num_test, num_examples))\n",
    "\n",
    "BATCH_SIZE=64\n",
    "train_dataloader = GraphDataLoader(\n",
    "    dataset, sampler=train_sampler, batch_size=BATCH_SIZE, drop_last=False)\n",
    "test_dataloader = GraphDataLoader(\n",
    "    dataset, sampler=test_sampler, batch_size=BATCH_SIZE, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl.nn.pytorch as dglnn\n",
    "import torch.nn as nn\n",
    "\n",
    "class RGCN(nn.Module):\n",
    "    def __init__(self, in_feats, hid_feats, out_feats, rel_names):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = dglnn.HeteroGraphConv({\n",
    "            rel: dglnn.GraphConv(in_feats, hid_feats)\n",
    "            for rel in rel_names}, aggregate='sum')\n",
    "        \n",
    "        self.conv2 = dglnn.HeteroGraphConv({\n",
    "            rel: dglnn.GraphConv(hid_feats, out_feats)\n",
    "            for rel in rel_names}, aggregate='sum')\n",
    "\n",
    "    def forward(self, graph, inputs):\n",
    "        # inputs is features of nodes\n",
    "        h = self.conv1(graph, inputs)\n",
    "        h = {k: F.relu(v) for k, v in h.items()}\n",
    "        h = self.conv2(graph, h)\n",
    "        return h\n",
    "\n",
    "class HeteroClassifier(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, n_classes, rel_names):\n",
    "        super().__init__()\n",
    "\n",
    "        self.rgcn = RGCN(in_dim, hidden_dim, hidden_dim, rel_names)\n",
    "        self.classify = nn.Linear(hidden_dim, n_classes)\n",
    "\n",
    "    def forward(self, g):\n",
    "        h = g.ndata['features']\n",
    "        h = self.rgcn(g, h)\n",
    "        with g.local_scope():\n",
    "            g.ndata['h'] = h\n",
    "            # Calculate graph representation by average readout.\n",
    "            hg = 0\n",
    "            for ntype in g.ntypes:\n",
    "                hg = hg + dgl.mean_nodes(g, 'h', ntype=ntype)\n",
    "            return self.classify(hg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:01<00:00, 14.00it/s]\n",
      " 12%|█▏        | 2/17 [00:00<00:01, 14.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0    Train accuracy: 0.527643064985451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:01<00:00, 15.65it/s]\n",
      " 12%|█▏        | 2/17 [00:00<00:01, 12.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1    Train accuracy: 0.5407371483996121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:01<00:00, 15.24it/s]\n",
      " 12%|█▏        | 2/17 [00:00<00:01, 13.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2    Train accuracy: 0.553831231813773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:01<00:00, 16.48it/s]\n",
      " 12%|█▏        | 2/17 [00:00<00:01, 13.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3    Train accuracy: 0.5521338506304558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:01<00:00, 13.29it/s]\n",
      " 12%|█▏        | 2/17 [00:00<00:01, 10.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4    Train accuracy: 0.5524733268671193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:01<00:00, 15.80it/s]\n",
      " 12%|█▏        | 2/17 [00:00<00:01, 13.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5    Train accuracy: 0.5478499838344649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:01<00:00, 15.66it/s]\n",
      " 12%|█▏        | 2/17 [00:00<00:01, 13.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6    Train accuracy: 0.5492586947485104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:01<00:00, 15.52it/s]\n",
      " 12%|█▏        | 2/17 [00:00<00:00, 16.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7    Train accuracy: 0.5493452958292919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:01<00:00, 15.71it/s]\n",
      " 12%|█▏        | 2/17 [00:00<00:00, 16.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8    Train accuracy: 0.5455329238064447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:01<00:00, 16.26it/s]\n",
      " 12%|█▏        | 2/17 [00:00<00:00, 15.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9    Train accuracy: 0.5431619786614937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:01<00:00, 15.19it/s]\n",
      " 12%|█▏        | 2/17 [00:00<00:00, 18.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10    Train accuracy: 0.5467771801428446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:01<00:00, 13.97it/s]\n",
      " 12%|█▏        | 2/17 [00:00<00:01, 12.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11    Train accuracy: 0.545021015195603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:01<00:00, 15.27it/s]\n",
      " 12%|█▏        | 2/17 [00:00<00:01, 14.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12    Train accuracy: 0.5476385883757368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:01<00:00, 13.74it/s]\n",
      " 12%|█▏        | 2/17 [00:00<00:01, 13.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13    Train accuracy: 0.5478037965913815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:01<00:00, 13.77it/s]\n",
      " 12%|█▏        | 2/17 [00:00<00:01, 10.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14    Train accuracy: 0.5485935984481086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:01<00:00, 13.97it/s]\n",
      " 12%|█▏        | 2/17 [00:00<00:00, 16.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15    Train accuracy: 0.5462536372453928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:01<00:00, 15.59it/s]\n",
      "  0%|          | 0/17 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16    Train accuracy: 0.546984652250813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:01<00:00, 14.44it/s]\n",
      " 12%|█▏        | 2/17 [00:00<00:01, 13.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17    Train accuracy: 0.5433236340122858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:01<00:00, 14.68it/s]\n",
      "  6%|▌         | 1/17 [00:00<00:02,  6.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18    Train accuracy: 0.543417223952218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:01<00:00, 12.59it/s]\n",
      "  0%|          | 0/17 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19    Train accuracy: 0.5432589718719689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:02<00:00,  6.72it/s]\n",
      "  0%|          | 0/17 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20    Train accuracy: 0.5440395362800794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:01<00:00, 14.57it/s]\n",
      " 12%|█▏        | 2/17 [00:00<00:00, 15.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21    Train accuracy: 0.5428092760779473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:02<00:00,  8.10it/s]\n",
      " 12%|█▏        | 2/17 [00:00<00:01, 14.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22    Train accuracy: 0.5443427655716274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:01<00:00, 14.15it/s]\n",
      " 12%|█▏        | 2/17 [00:00<00:01, 14.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23    Train accuracy: 0.5439702554154543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:01<00:00, 15.38it/s]\n",
      " 12%|█▏        | 2/17 [00:00<00:00, 16.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24    Train accuracy: 0.5441707080504364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:01<00:00, 13.93it/s]\n",
      " 12%|█▏        | 2/17 [00:00<00:00, 16.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25    Train accuracy: 0.545064537790047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:02<00:00,  8.07it/s]\n",
      " 12%|█▏        | 2/17 [00:00<00:01, 13.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26    Train accuracy: 0.5440959873549592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:01<00:00, 13.22it/s]\n",
      " 12%|█▏        | 2/17 [00:00<00:01, 13.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27    Train accuracy: 0.5442011916308716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:01<00:00, 13.78it/s]\n",
      "  6%|▌         | 1/17 [00:00<00:01,  9.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28    Train accuracy: 0.5442991404394796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:01<00:00, 13.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29    Train accuracy: 0.543873262204979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# etypes is the list of edge types as strings.\n",
    "model = HeteroClassifier(10, 256, 2, etypes)\n",
    "opt = torch.optim.Adam(model.parameters())\n",
    "num_correct = 0\n",
    "num_tests = 0\n",
    "\n",
    "for epoch in range(30):\n",
    "    for batched_graph, labels in tqdm(train_dataloader):\n",
    "        logits = model(batched_graph)\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        num_correct += (logits.argmax(1) == labels).sum().item()\n",
    "        num_tests += len(labels)\n",
    "    print('Epoch {}    Train accuracy: {}'.format(epoch, num_correct / num_tests))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl.function as fn\n",
    "\n",
    "class HeteroRGCNLayer(nn.Module):\n",
    "    def __init__(self, in_size, out_size, etypes):\n",
    "        super(HeteroRGCNLayer, self).__init__()\n",
    "        # W_r for each relation\n",
    "        self.weight = nn.ModuleDict({\n",
    "                name : nn.Linear(in_size, out_size) for name in etypes\n",
    "            })\n",
    "\n",
    "    def forward(self, G, feat_dict):\n",
    "        # The input is a dictionary of node features for each type\n",
    "        funcs = {}\n",
    "        for srctype, etype, dsttype in G.canonical_etypes:\n",
    "            # Compute W_r * h\n",
    "            Wh = self.weight[etype](feat_dict[srctype])\n",
    "            # Save it in graph for message passing\n",
    "            G.nodes[srctype].data['Wh_%s' % etype] = Wh\n",
    "            # Specify per-relation message passing functions: (message_func, reduce_func).\n",
    "            # Note that the results are saved to the same destination feature 'h', which\n",
    "            # hints the type wise reducer for aggregation.\n",
    "            funcs[etype] = (fn.copy_u('Wh_%s' % etype, 'm'), fn.mean('m', 'h'))\n",
    "        # Trigger message passing of multiple types.\n",
    "        # The first argument is the message passing functions for each relation.\n",
    "        # The second one is the type wise reducer, could be \"sum\", \"max\",\n",
    "        # \"min\", \"mean\", \"stack\"\n",
    "        G.multi_update_all(funcs, 'sum')\n",
    "        # return the updated node feature dictionary\n",
    "        return {ntype : G.nodes[ntype].data['h'] for ntype in G.ntypes}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeteroRGCN(nn.Module):\n",
    "    def __init__(self, G, in_size, hidden_size, out_size):\n",
    "        super(HeteroRGCN, self).__init__()\n",
    "        # Use trainable node embeddings as featureless inputs.\n",
    "        embed_dict = {ntype : nn.Parameter(torch.Tensor(G.number_of_nodes(ntype), in_size))\n",
    "                      for ntype in G.ntypes}\n",
    "        for key, embed in embed_dict.items():\n",
    "            nn.init.xavier_uniform_(embed)\n",
    "        self.embed = nn.ParameterDict(embed_dict)\n",
    "        # create layers\n",
    "        self.layer1 = HeteroRGCNLayer(in_size, hidden_size, G.etypes)\n",
    "        self.layer2 = HeteroRGCNLayer(hidden_size, out_size, G.etypes)\n",
    "\n",
    "    def forward(self, G):\n",
    "        h_dict = self.layer1(G, self.embed)\n",
    "        h_dict = {k : F.leaky_relu(h) for k, h in h_dict.items()}\n",
    "        h_dict = self.layer2(G, h_dict)\n",
    "        # get paper logits\n",
    "        return h_dict['paper']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'G' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-b34b96d8d544>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHeteroRGCN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5e-4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mbest_val_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'G' is not defined"
     ]
    }
   ],
   "source": [
    "model = HeteroRGCN(G, 10, 10, 3)\n",
    "\n",
    "opt = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "best_val_acc = 0\n",
    "best_test_acc = 0\n",
    "\n",
    "for epoch in range(100):\n",
    "    logits = model(G)\n",
    "    # The loss is computed only for labeled nodes.\n",
    "    loss = F.cross_entropy(logits[train_idx], labels[train_idx])\n",
    "\n",
    "    pred = logits.argmax(1)\n",
    "    train_acc = (pred[train_idx] == labels[train_idx]).float().mean()\n",
    "    val_acc = (pred[val_idx] == labels[val_idx]).float().mean()\n",
    "    test_acc = (pred[test_idx] == labels[test_idx]).float().mean()\n",
    "\n",
    "    if best_val_acc < val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_test_acc = test_acc\n",
    "\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        print('Loss %.4f, Train Acc %.4f, Val Acc %.4f (Best %.4f), Test Acc %.4f (Best %.4f)' % (\n",
    "            loss.item(),\n",
    "            train_acc.item(),\n",
    "            val_acc.item(),\n",
    "            best_val_acc.item(),\n",
    "            test_acc.item(),\n",
    "            best_test_acc.item(),\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import numpy as np\n",
    "import torch as th\n",
    "from dgl.nn import RelGraphConv\n",
    "\n",
    "g = dgl.graph(([0,1,2,3,2,5], [1,2,3,4,0,3]))\n",
    "feat = th.ones(6, 10)\n",
    "conv = RelGraphConv(10, 2, 3, regularizer='basis', num_bases=2)\n",
    "etype = th.tensor(np.array([0,1,2,0,1,2]).astype(np.int64))\n",
    "res = conv(g, feat, etype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = [0, 1, 0, 0, 1]\n",
    "v = [0, 1, 2, 3, 2]\n",
    "g = dgl.heterograph({('_U', '_E', '_V') : (u, v)})\n",
    "u_fea = th.rand(2, 5)\n",
    "v_fea = th.rand(4, 5)\n",
    "conv = GraphConv(5, 2, norm='both', weight=True, bias=True)\n",
    "res = conv(g, (u_fea, v_fea))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading /home/christinedk/.dgl/aifb.tgz from https://data.dgl.ai/dataset/aifb.tgz...\n",
      "Extracting file to /home/christinedk/.dgl/aifb\n",
      "Loading dataset aifb\n",
      "Graph loaded, frequencies counted.\n",
      "Number of nodes:  8285\n",
      "Number of relations:  91\n",
      "Number of edges:  66371\n",
      "4 classes: {'http://www.aifb.uni-karlsruhe.de/Forschungsgruppen/viewForschungsgruppeOWL/id2instance', 'http://www.aifb.uni-karlsruhe.de/Forschungsgruppen/viewForschungsgruppeOWL/id3instance', 'http://www.aifb.uni-karlsruhe.de/Forschungsgruppen/viewForschungsgruppeOWL/id1instance', 'http://www.aifb.uni-karlsruhe.de/Forschungsgruppen/viewForschungsgruppeOWL/id4instance'}\n",
      "Loading training set\n",
      "Loading test set\n",
      "Number of classes:  4\n",
      "removing nodes that are more than 3 hops away\n"
     ]
    }
   ],
   "source": [
    "from dgl.contrib.data import load_data\n",
    "data = load_data(dataset='aifb')\n",
    "num_nodes = data.num_nodes\n",
    "num_rels = data.num_rels\n",
    "num_classes = data.num_classes\n",
    "labels = data.labels\n",
    "train_idx = data.train_idx\n",
    "# split training and validation set\n",
    "val_idx = train_idx[:len(train_idx) // 5]\n",
    "train_idx = train_idx[len(train_idx) // 5:]\n",
    "\n",
    "# edge type and normalization factor\n",
    "edge_type = torch.from_numpy(data.edge_type)\n",
    "edge_norm = torch.from_numpy(data.edge_norm).unsqueeze(1)\n",
    "\n",
    "labels = torch.from_numpy(labels).view(-1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
