{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import dgl.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading /home/christinedk/.dgl/GINDataset.zip from https://raw.githubusercontent.com/weihua916/powerful-gnns/master/dataset.zip...\n",
      "Extracting file to /home/christinedk/.dgl/GINDataset\n"
     ]
    }
   ],
   "source": [
    "# Generate a synthetic dataset with 10000 graphs, ranging from 10 to 500 nodes.\n",
    "dataset = dgl.data.GINDataset('PROTEINS', self_loop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node feature dimensionality: 3\n",
      "Number of graph categories: 2\n"
     ]
    }
   ],
   "source": [
    "print('Node feature dimensionality:', dataset.dim_nfeats)\n",
    "print('Number of graph categories:', dataset.gclasses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.dataloading import GraphDataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "num_examples = len(dataset)\n",
    "num_train = int(num_examples * 0.8)\n",
    "\n",
    "train_sampler = SubsetRandomSampler(torch.arange(num_train))\n",
    "test_sampler = SubsetRandomSampler(torch.arange(num_train, num_examples))\n",
    "\n",
    "train_dataloader = GraphDataLoader(\n",
    "    dataset, sampler=train_sampler, batch_size=5, drop_last=False)\n",
    "test_dataloader = GraphDataLoader(\n",
    "    dataset, sampler=test_sampler, batch_size=5, drop_last=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Graph(num_nodes=279, num_edges=1319,\n",
      "      ndata_schemes={'label': Scheme(shape=(), dtype=torch.int64), 'attr': Scheme(shape=(3,), dtype=torch.float32)}\n",
      "      edata_schemes={}), tensor([0, 0, 0, 0, 0])]\n"
     ]
    }
   ],
   "source": [
    "it = iter(train_dataloader)\n",
    "batch = next(it)\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes for each graph element in the batch: tensor([84,  7, 68, 36, 84])\n",
      "Number of edges for each graph element in the batch: tensor([438,  33, 354, 174, 320])\n",
      "The original graphs in the minibatch:\n",
      "[Graph(num_nodes=84, num_edges=438,\n",
      "      ndata_schemes={'label': Scheme(shape=(), dtype=torch.int64), 'attr': Scheme(shape=(3,), dtype=torch.float32)}\n",
      "      edata_schemes={}), Graph(num_nodes=7, num_edges=33,\n",
      "      ndata_schemes={'label': Scheme(shape=(), dtype=torch.int64), 'attr': Scheme(shape=(3,), dtype=torch.float32)}\n",
      "      edata_schemes={}), Graph(num_nodes=68, num_edges=354,\n",
      "      ndata_schemes={'label': Scheme(shape=(), dtype=torch.int64), 'attr': Scheme(shape=(3,), dtype=torch.float32)}\n",
      "      edata_schemes={}), Graph(num_nodes=36, num_edges=174,\n",
      "      ndata_schemes={'label': Scheme(shape=(), dtype=torch.int64), 'attr': Scheme(shape=(3,), dtype=torch.float32)}\n",
      "      edata_schemes={}), Graph(num_nodes=84, num_edges=320,\n",
      "      ndata_schemes={'label': Scheme(shape=(), dtype=torch.int64), 'attr': Scheme(shape=(3,), dtype=torch.float32)}\n",
      "      edata_schemes={})]\n"
     ]
    }
   ],
   "source": [
    "batched_graph, labels = batch\n",
    "print('Number of nodes for each graph element in the batch:', batched_graph.batch_num_nodes())\n",
    "print('Number of edges for each graph element in the batch:', batched_graph.batch_num_edges())\n",
    "\n",
    "# Recover the original graph elements from the minibatch\n",
    "graphs = dgl.unbatch(batched_graph)\n",
    "print('The original graphs in the minibatch:')\n",
    "print(graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.nn import GraphConv\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_feats, h_feats, num_classes):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GraphConv(in_feats, h_feats)\n",
    "        self.conv2 = GraphConv(h_feats, num_classes)\n",
    "\n",
    "    def forward(self, g, in_feat):\n",
    "        h = self.conv1(g, in_feat)\n",
    "        h = F.relu(h)\n",
    "        h = self.conv2(g, h)\n",
    "        g.ndata['h'] = h\n",
    "        return dgl.mean_nodes(g, 'h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.2914798206278027\n"
     ]
    }
   ],
   "source": [
    "# Create the model with given dimensions\n",
    "model = GCN(dataset.dim_nfeats, 16, dataset.gclasses)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(20):\n",
    "    for batched_graph, labels in train_dataloader:\n",
    "        pred = model(batched_graph, batched_graph.ndata['attr'].float())\n",
    "        loss = F.cross_entropy(pred, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "num_correct = 0\n",
    "num_tests = 0\n",
    "for batched_graph, labels in test_dataloader:\n",
    "    pred = model(batched_graph, batched_graph.ndata['attr'].float())\n",
    "    num_correct += (pred.argmax(1) == labels).sum().item()\n",
    "    num_tests += len(labels)\n",
    "\n",
    "print('Test accuracy:', num_correct / num_tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making my own dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Src</th>\n",
       "      <th>Dst</th>\n",
       "      <th>Weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.043591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.282119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.370293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.730570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.821187</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Src  Dst    Weight\n",
       "0    0    1  0.043591\n",
       "1    0    2  0.282119\n",
       "2    0    3  0.370293\n",
       "3    0    4  0.730570\n",
       "4    0    5  0.821187"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "import pandas as pd\n",
    "urllib.request.urlretrieve(\n",
    "    'https://data.dgl.ai/tutorial/dataset/members.csv', './members.csv')\n",
    "urllib.request.urlretrieve(\n",
    "    'https://data.dgl.ai/tutorial/dataset/interactions.csv', './interactions.csv')\n",
    "\n",
    "members = pd.read_csv('./members.csv')\n",
    "members.head()\n",
    "\n",
    "interactions = pd.read_csv('./interactions.csv')\n",
    "interactions.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph(num_nodes=34, num_edges=156,\n",
      "      ndata_schemes={'feat': Scheme(shape=(), dtype=torch.int64), 'label': Scheme(shape=(), dtype=torch.int8), 'train_mask': Scheme(shape=(), dtype=torch.bool), 'val_mask': Scheme(shape=(), dtype=torch.bool), 'test_mask': Scheme(shape=(), dtype=torch.bool)}\n",
      "      edata_schemes={'weight': Scheme(shape=(), dtype=torch.float64)})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/christinedk/venv/lib/python3.7/site-packages/ipykernel_launcher.py:14: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:143.)\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "from dgl.data import DGLDataset\n",
    "import torch\n",
    "import os\n",
    "\n",
    "class KarateClubDataset(DGLDataset):\n",
    "    def __init__(self):\n",
    "        super().__init__(name='karate_club')\n",
    "\n",
    "    def process(self):\n",
    "        nodes_data = pd.read_csv('./members.csv')\n",
    "        edges_data = pd.read_csv('./interactions.csv')\n",
    "        node_features = torch.from_numpy(nodes_data['Age'].to_numpy())\n",
    "        node_labels = torch.from_numpy(nodes_data['Club'].astype('category').cat.codes.to_numpy())\n",
    "        edge_features = torch.from_numpy(edges_data['Weight'].to_numpy())\n",
    "        edges_src = torch.from_numpy(edges_data['Src'].to_numpy())\n",
    "        edges_dst = torch.from_numpy(edges_data['Dst'].to_numpy())\n",
    "\n",
    "        self.graph = dgl.graph((edges_src, edges_dst), num_nodes=nodes_data.shape[0])\n",
    "        self.graph.ndata['feat'] = node_features\n",
    "        self.graph.ndata['label'] = node_labels\n",
    "        self.graph.edata['weight'] = edge_features\n",
    "\n",
    "        # If your dataset is a node classification dataset, you will need to assign\n",
    "        # masks indicating whether a node belongs to training, validation, and test set.\n",
    "        n_nodes = nodes_data.shape[0]\n",
    "        n_train = int(n_nodes * 0.6)\n",
    "        n_val = int(n_nodes * 0.2)\n",
    "        train_mask = torch.zeros(n_nodes, dtype=torch.bool)\n",
    "        val_mask = torch.zeros(n_nodes, dtype=torch.bool)\n",
    "        test_mask = torch.zeros(n_nodes, dtype=torch.bool)\n",
    "        train_mask[:n_train] = True\n",
    "        val_mask[n_train:n_train + n_val] = True\n",
    "        test_mask[n_train + n_val:] = True\n",
    "        self.graph.ndata['train_mask'] = train_mask\n",
    "        self.graph.ndata['val_mask'] = val_mask\n",
    "        self.graph.ndata['test_mask'] = test_mask\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.graph\n",
    "\n",
    "    def __len__(self):\n",
    "        return 1\n",
    "\n",
    "dataset = KarateClubDataset()\n",
    "graph = dataset[0]\n",
    "\n",
    "print(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph(num_nodes=15, num_edges=45,\n",
      "      ndata_schemes={}\n",
      "      edata_schemes={}) tensor(0)\n"
     ]
    }
   ],
   "source": [
    "urllib.request.urlretrieve(\n",
    "    'https://data.dgl.ai/tutorial/dataset/graph_edges.csv', './graph_edges.csv')\n",
    "urllib.request.urlretrieve(\n",
    "    'https://data.dgl.ai/tutorial/dataset/graph_properties.csv', './graph_properties.csv')\n",
    "edges = pd.read_csv('./graph_edges.csv')\n",
    "properties = pd.read_csv('./graph_properties.csv')\n",
    "\n",
    "edges.head()\n",
    "\n",
    "properties.head()\n",
    "\n",
    "class SyntheticDataset(DGLDataset):\n",
    "    def __init__(self):\n",
    "        super().__init__(name='synthetic')\n",
    "\n",
    "    def process(self):\n",
    "        edges = pd.read_csv('./graph_edges.csv')\n",
    "        properties = pd.read_csv('./graph_properties.csv')\n",
    "        self.graphs = []\n",
    "        self.labels = []\n",
    "\n",
    "        # Create a graph for each graph ID from the edges table.\n",
    "        # First process the properties table into two dictionaries with graph IDs as keys.\n",
    "        # The label and number of nodes are values.\n",
    "        label_dict = {}\n",
    "        num_nodes_dict = {}\n",
    "        for _, row in properties.iterrows():\n",
    "            label_dict[row['graph_id']] = row['label']\n",
    "            num_nodes_dict[row['graph_id']] = row['num_nodes']\n",
    "\n",
    "        # For the edges, first group the table by graph IDs.\n",
    "        edges_group = edges.groupby('graph_id')\n",
    "\n",
    "        # For each graph ID...\n",
    "        for graph_id in edges_group.groups:\n",
    "            # Find the edges as well as the number of nodes and its label.\n",
    "            edges_of_id = edges_group.get_group(graph_id)\n",
    "            src = edges_of_id['src'].to_numpy()\n",
    "            dst = edges_of_id['dst'].to_numpy()\n",
    "            num_nodes = num_nodes_dict[graph_id]\n",
    "            label = label_dict[graph_id]\n",
    "\n",
    "            # Create a graph and add it to the list of graphs and labels.\n",
    "            g = dgl.graph((src, dst), num_nodes=num_nodes)\n",
    "            self.graphs.append(g)\n",
    "            self.labels.append(label)\n",
    "\n",
    "        # Convert the label list to tensor for saving.\n",
    "        self.labels = torch.LongTensor(self.labels)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.graphs[i], self.labels[i]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.graphs)\n",
    "\n",
    "dataset = SyntheticDataset()\n",
    "graph, label = dataset[0]\n",
    "print(graph, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# heterograph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import numpy as np\n",
    "\n",
    "ratings = dgl.heterograph(\n",
    "    {('user', '+1', 'movie') : (np.array([0, 0, 1]), np.array([0, 1, 0])),\n",
    "     ('user', '-1', 'movie') : (np.array([2]), np.array([1]))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = dgl.heterograph({\n",
    "     ('user', 'follows', 'user'): [(0, 1), (1, 2)],\n",
    "     ('user', 'plays', 'game'): [(0, 0), (1, 0), (1, 1), (2, 1)],\n",
    "     ('developer', 'develops', 'game'): [(0, 0), (1, 1)],\n",
    "     })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.number_of_nodes('user')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__header__', '__version__', '__globals__', 'TvsP', 'PvsA', 'PvsV', 'AvsF', 'VvsC', 'PvsL', 'PvsC', 'A', 'C', 'F', 'L', 'P', 'T', 'V', 'PvsT', 'CNormPvsA', 'RNormPvsA', 'CNormPvsC', 'RNormPvsC', 'CNormPvsT', 'RNormPvsT', 'CNormPvsV', 'RNormPvsV', 'CNormVvsC', 'RNormVvsC', 'CNormAvsF', 'RNormAvsF', 'CNormPvsL', 'RNormPvsL', 'stopwords', 'nPvsT', 'nT', 'CNormnPvsT', 'RNormnPvsT', 'nnPvsT', 'nnT', 'CNormnnPvsT', 'RNormnnPvsT', 'PvsP', 'CNormPvsP', 'RNormPvsP']\n"
     ]
    }
   ],
   "source": [
    "import scipy.io\n",
    "import urllib.request\n",
    "\n",
    "data_url = 'https://data.dgl.ai/dataset/ACM.mat'\n",
    "data_file_path = '/tmp/ACM.mat'\n",
    "\n",
    "urllib.request.urlretrieve(data_url, data_file_path)\n",
    "data = scipy.io.loadmat(data_file_path)\n",
    "print(list(data.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['PvsA'].it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "nz = [(i, data['PvsA'][i,j], j) for i, j in zip(*data['PvsA'].nonzero())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "nz = [(i, j) for i, j in zip(*data['PvsA'].nonzero())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = dgl.heterograph({('paper', 'written-by', 'author'):nz})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for today: construct a heterograph with weight features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.ndata['x'] = {'paper':th.ones(graph.num_nodes('paper'), 3)}               # node feature of length 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = th.tensor([0, 0, 0, 1]), th.tensor([1, 2, 3, 3])\n",
    "weights = th.tensor([0.1, 0.6, 0.9, 0.7])  # weight of each edge\n",
    "g = dgl.graph(edges)\n",
    "g.edata['w'] = weights  # give it a name 'w'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Graph(num_nodes=4, num_edges=4,\n",
       "      ndata_schemes={}\n",
       "      edata_schemes={'w': Scheme(shape=(), dtype=torch.float32)})"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/srv/home/christinedk/wp_internship/features/activity_fanpov.json','rb') as f:\n",
    "    page_history = json.load(f)\n",
    "\n",
    "with open('/srv/home/christinedk/wp_internship/features/talk_fanpov.json','rb') as f:\n",
    "    talk_history = json.load(f)\n",
    "    c\n",
    "with open('/srv/home/christinedk/wp_internship/features/editorsfanpov_v2.json','rb') as f:\n",
    "    editor_history = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_page = page_history[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_collab = editor_history[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "editor_nodes = [d['event_user_id'] for d in sample_page['user_article']]\n",
    "editor_to_ind = {j:i for i,j in enumerate(editor_nodes)}\n",
    "editor_page_links = [(editor_to_ind[i],0) for i in editor_nodes]\n",
    "collab_links_directed = [(editor_to_ind[pair['event_user_id']],editor_to_ind[pair['event_user_id_r']]) \n",
    "                             for pair in sample_collab['collaboration']['directed']]\n",
    "collab_links_undirected = [(editor_to_ind[d['pair'][0]],editor_to_ind[d['pair'][1]]) \n",
    "                            for d in sample_collab['collaboration']['undirected']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = dgl.heterograph({\n",
    "     ('editor', 'edits', 'page'): editor_page_links,\n",
    "     ('editor', 'collab-dir', 'editor'): collab_links_directed,\n",
    "     ('editor', 'collab-undir', 'editor'): collab_links_undirected + [(j,i) for i,j in collab_links_undirected]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EDITOR_FEATURES = 10\n",
    "NUM_PAGE_FEATURES = 15\n",
    "NUM_EDITOR_PAGE_FEATURES = 8\n",
    "NUM_COLLAB_DIR_FEAT = 2\n",
    "NUM_COLLAB_UNDIR_FEAT = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_features = th.tensor([list(sample_page['article'].values())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "editor_article_features_lookup = pd.DataFrame(sample_page['user_article'])\\\n",
    "                                        .set_index('event_user_id').to_dict('index')\n",
    "null_dict = {i: 0 for i in range(NUM_EDITOR_PAGE_FEATURES)}\n",
    "editor_article_features = th.tensor([list(editor_article_features_lookup.get(i,null_dict).values()) \n",
    "                                     for i,_ in editor_page_links])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "editor_features_lookup = pd.DataFrame(sample_collab['editor'])\\\n",
    "                                    .set_index('event_user_id').to_dict('index')\n",
    "null_dict = {i: 0 for i in range(NUM_EDITOR_FEATURES)}\n",
    "editor_features = th.tensor([list(editor_features_lookup.get(i,null_dict).values()) for i in editor_nodes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "collab_dir_lookup = pd.DataFrame(sample_collab['collaboration']['directed'])\\\n",
    "                        .set_index(['event_user_id','event_user_id_r']).to_dict('index')\n",
    "null_dict = {i: 0 for i in range(NUM_COLLAB_DIR_FEAT)}\n",
    "collab_dir_features = th.tensor([list(collab_dir_lookup.get((i,j),null_dict).values()) \n",
    "                                 for i,j in collab_links_directed])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "collab_undir_lookup = pd.DataFrame(sample_collab['collaboration']['undirected'])\n",
    "collab_undir_lookup = pd.concat([collab_undir_lookup,\n",
    "                                pd.DataFrame(collab_undir_lookup.pair.to_list(),columns=['id_1','id_2'])],axis=1)\\\n",
    "                            .set_index(['id_1','id_2'])\\\n",
    "                            .drop('pair',axis=1)\\\n",
    "                            .to_dict('index')\n",
    "\n",
    "null_dict = {i: 0 for i in range(NUM_COLLAB_UNDIR_FEAT)}\n",
    "collab_undir_features = [list(collab_undir_lookup.get((i,j),null_dict).values()) for i,j in collab_links_undirected]\n",
    "collab_undir_features = th.tensor(collab_undir_features + collab_undir_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.ndata['features'] = {'page':page_features,\n",
    "                            'editor':editor_features}\n",
    "\n",
    "g.edata['features'] = {'collab-dir':collab_dir_features,\n",
    "                       'collab-undir':collab_undir_features,\n",
    "                       'edits':editor_article_features}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
