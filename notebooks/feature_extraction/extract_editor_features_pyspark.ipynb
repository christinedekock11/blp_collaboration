{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "HOME = '/srv/home/christinedk/wp_internship/'\n",
    "DATA_DIR = HOME + 'data/'\n",
    "sys.path.append(HOME + 'collaboration/')\n",
    "from config import TEMPLATES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "from time import time\n",
    "from math import log2\n",
    "import json\n",
    "\n",
    "import dateutil \n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import ArrayType, StringType, IntegerType, DoubleType, FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def talk_article_ratio(page_namespace_list):\n",
    "    page_namespace_list = np.array(page_namespace_list)\n",
    "    talk = np.sum(page_namespace_list==1) + 1\n",
    "    article = np.sum(page_namespace_list==0) + 1\n",
    "    return float(article/talk)\n",
    "udf_page_talk_ratio = udf(talk_article_ratio,FloatType())\n",
    "\n",
    "def entropy(p):\n",
    "    return -sum([p[i] * log2(p[i]) for i in range(len(p))])\n",
    "\n",
    "def contribution_fracs(page_ids):\n",
    "    counts = np.unique(page_ids,return_counts=True)[1]\n",
    "    cf = counts/sum(counts)\n",
    "    entropy_cf = float(entropy(cf))\n",
    "    return entropy_cf\n",
    "udf_contribution_frac = udf(contribution_fracs,FloatType())\n",
    "\n",
    "def read_revisions(filename, rename=False):\n",
    "    revisions = pd.read_json(filename,lines=True)\n",
    "    if rename:\n",
    "        revisions = revisions.rename(columns = {'revision_timestamp':'event_timestamp','user_id':'event_user_id'})\n",
    "    revisions['event_timestamp'] = pd.to_datetime(revisions['event_timestamp'])\n",
    "    revisions = revisions.sort_values(by='event_timestamp', ascending=True)\n",
    "    return revisions\n",
    "\n",
    "def np_encoder(object):\n",
    "    if isinstance(object, np.generic):\n",
    "        return object.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_editor_features(tag_user_histories):\n",
    "    \n",
    "    tag_features = tag_user_histories.groupby('event_user_id')\\\n",
    "         .agg(f.last('num_groups').alias('num_groups'),\n",
    "           f.countDistinct('page_id').alias('num_articles'),\n",
    "           f.count('revision_id').alias('num_edits'),\n",
    "           f.last('num_blocks_historical').alias('num_past_blocks'),\n",
    "           f.last('num_curr_blocks').alias('num_curr_blocks'),\n",
    "           f.sum(col(\"is_revert_bool\")).alias('num_reverts_by_others'),\n",
    "           f.sum(col('is_reverted_bool')).alias('num_reverts_of_others'),\n",
    "           f.last('days_since_registration').alias('time_since_registration'),\n",
    "           udf_page_talk_ratio(f.collect_list('page_namespace')).alias('talk_article_ratio'),\n",
    "           udf_contribution_frac(f.collect_list('page_id')).alias('contribution_frac_entropy') \n",
    "          )\n",
    "    \n",
    "    return tag_features\n",
    "\n",
    "def get_user_interactions(user_histories_1year):\n",
    "    user_page_revisions = user_histories_1year.select(col('page_id'),col('event_user_id'),\n",
    "                                                      col('revision_id'),col('page_namespace'))\\\n",
    "                                .groupBy(\"page_id\",\"event_user_id\").agg(\n",
    "                                f.count(\"revision_id\").alias(\"revisions_count\"),\n",
    "                                f.first(\"page_namespace\").alias('page_namespace'))\n",
    "                                #f.first(\"concentration_ratio\").alias('concentration_ratio'))\n",
    "\n",
    "    self_join_df = user_page_revisions.toDF(*[c + '_r' for c in user_page_revisions.columns])\n",
    "    editor_interactions = user_page_revisions.join(self_join_df,[user_page_revisions.page_id == self_join_df.page_id_r,\n",
    "                           user_page_revisions.event_user_id != self_join_df.event_user_id_r]).drop('page_id_r')\n",
    "    \n",
    "    return editor_interactions\n",
    "\n",
    "def calculate_concentration_ratios(user_histories):\n",
    "    concentration_ratio = user_histories.groupby('page_id').agg(\n",
    "                                    f.countDistinct('event_user_id').alias('num_editors'),\n",
    "                                    f.count('event_user_id').alias('num_revisions'))\\\n",
    "                                    .withColumn('concentration_ratio',col('num_editors')/col('num_revisions'))\n",
    "\n",
    "    user_histories = user_histories.join(concentration_ratio,on='page_id')\n",
    "    return user_histories\n",
    "\n",
    "def get_directed_features(paired_interactions):\n",
    "    user_article_edits = paired_interactions.groupby('event_user_id')\\\n",
    "                             .agg(f.sum('num_common_articles').alias('editor_pages_total'),\n",
    "                             f.sum('num_revisions_articles').alias('editor_revisions_total'))\n",
    "\n",
    "    directed = paired_interactions\\\n",
    "        .join(user_article_edits.select('event_user_id','editor_pages_total','editor_revisions_total'), on=\"event_user_id\")\\\n",
    "        .withColumn(\"coedit_ratio\", (col(\"num_common_articles\") / col(\"editor_pages_total\")))\\\n",
    "        .withColumn('coedit_revisions_ratio', (col('num_revisions_articles') / col('editor_revisions_total')))\\\n",
    "        .select('event_user_id','event_user_id_r','coedit_ratio', 'coedit_revisions_ratio')  \n",
    "    \n",
    "    return directed\n",
    "\n",
    "\n",
    "def get_undirected_features(paired_interactions, paired_interactions_articles):\n",
    "    features_all = paired_interactions.withColumn('pair',f.array_sort(f.array(col('event_user_id'),col('event_user_id_r'))))\\\n",
    "                            .drop_duplicates(subset=['pair'])\\\n",
    "                            .select('pair','num_common_pages')\n",
    "    features_articles = paired_interactions_articles.withColumn('pair',f.array_sort(f.array(col('event_user_id'),col('event_user_id_r'))))\\\n",
    "                                    .drop_duplicates(subset=['pair'])\\\n",
    "                                    .select('pair','num_common_articles')#,'mean_concentration_ratio')\n",
    "\n",
    "    undirected = features_all.join(features_articles,on='pair')\n",
    "    return undirected\n",
    "\n",
    "def calculate_collaboration_features(editor_interactions):\n",
    "    #editor_interactions = calculate_concentration_ratios(editor_interactions)\n",
    "    \n",
    "    paired_interactions_articles = editor_interactions.filter(col('page_namespace')==0)\\\n",
    "                            .groupby('event_user_id','event_user_id_r')\\\n",
    "                            .agg(f.count(\"page_id\").alias('num_common_articles'),\n",
    "                                 f.sum('revisions_count').alias('num_revisions_articles'))\\\n",
    "                            .cache()\n",
    "                            #f.mean('concentration_ratio').alias('mean_concentration_ratio')\n",
    "                            #.filter(col('num_common_articles')>=5)\n",
    "    \n",
    "    paired_interactions_all = editor_interactions.groupby('event_user_id','event_user_id_r')\\\n",
    "                                             .agg(f.count(\"page_id\").alias('num_common_pages'))\n",
    "    \n",
    "    directed_features = get_directed_features(paired_interactions_articles)\n",
    "    undirected_features = get_undirected_features(paired_interactions_all, paired_interactions_articles)\n",
    "                                                  \n",
    "    return directed_features,undirected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "user_histories = spark.read.parquet('editors/user_histories.parquet')\n",
    "user_histories = user_histories.withColumn('event_timestamp',\n",
    "                                           f.to_timestamp(col('event_timestamp')))\\\n",
    "                                .withColumn('event_user_registration_timestamp',\n",
    "                                           f.to_timestamp(col('event_user_registration_timestamp')))\\\n",
    "                                .withColumn('is_revert_bool',col(\"revision_is_identity_revert\").cast(\"long\"))\\\n",
    "                                .withColumn('is_reverted_bool',col(\"revision_is_identity_reverted\").cast(\"long\"))\n",
    "\n",
    "user_histories = user_histories.orderBy(col('event_timestamp'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for template in ['peacock']:\n",
    "    print(template)\n",
    "    t1=time()\n",
    "    labels = pd.read_csv(DATA_DIR + 'labels/{}.csv'.format(template))\n",
    "    labels['event_timestamp'] = pd.to_datetime(labels['event_timestamp'])\n",
    "\n",
    "    revisions = read_revisions(DATA_DIR + 'page_history/page_history-{}-meta-info.json'.format(template))\n",
    "\n",
    "    features = []\n",
    "    pages = revisions.groupby('page_id')\n",
    "\n",
    "    print('total: ',len(labels))\n",
    "    i=0\n",
    "    for tag_date, page_id in labels.values:\n",
    "        print('processing :',i)\n",
    "        i+=1\n",
    "\n",
    "        page_revisions = pages.get_group(page_id)\n",
    "        \n",
    "        # I think this can be done in a better way\n",
    "        tag_users = page_revisions[page_revisions.event_timestamp <= tag_date]\\\n",
    "                                    .event_user_id.dropna().unique().tolist()\n",
    "        tag_user_histories = user_histories.filter(col('event_timestamp')<=tag_date)\\\n",
    "                                           .filter(col('event_user_id').isin(tag_users))\\\n",
    "                                           .withColumn('days_since_registration',f.datediff(f.lit(tag_date),col('event_user_registration_timestamp')))\n",
    "        #tag_user_histores = calculate_concentration_ratios(tag_user_histories)\\\n",
    "\n",
    "\n",
    "        editor_features = get_editor_features(tag_user_histories).toPandas().to_dict('records')\n",
    "\n",
    "        start_date = tag_date - relativedelta(years=1)\n",
    "        user_histories_1year = tag_user_histories.filter(col(\"event_timestamp\").between(start_date,tag_date))\n",
    "        editor_interactions = get_user_interactions(user_histories_1year)    \n",
    "\n",
    "        directed_features, undirected_features = calculate_collaboration_features(editor_interactions)\n",
    "        collaboration = {'directed':directed_features.toPandas().to_dict('records'), \n",
    "                         'undirected':undirected_features.toPandas().to_dict('records')}\n",
    "\n",
    "        features.append({'date':str(tag_date),'page_id':page_id,'editor':editor_features,\n",
    "                         'collaboration':collaboration})\n",
    "        \n",
    "    with open(HOME +'features/editors'+template+'.json','w') as file:\n",
    "        json.dump(features,file,default=np_encoder)\n",
    "    \n",
    "    print('time: ',int(time()-t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(HOME +'features/editors'+template+'.json','w') as file:\n",
    "    json.dump(features,file,default=np_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "user_histories = spark.read.parquet('editors/user_histories.parquet')\n",
    "user_histories = user_histories.withColumn('event_timestamp',\n",
    "                                           f.to_timestamp(col('event_timestamp')))\\\n",
    "                                .withColumn('event_user_registration_timestamp',\n",
    "                                           f.to_timestamp(col('event_user_registration_timestamp')))\\\n",
    "                                .withColumn('is_revert_bool',col(\"revision_is_identity_revert\").cast(\"long\"))\\\n",
    "                                .withColumn('is_reverted_bool',col(\"revision_is_identity_reverted\").cast(\"long\"))\n",
    "\n",
    "user_histories = user_histories.orderBy(col('event_timestamp'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4286"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template='peacock'\n",
    "\n",
    "with open(HOME +'features/editors'+template+'_v2.json','rb') as file:\n",
    "    features = json.load(file)\n",
    "    \n",
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peacock\n"
     ]
    }
   ],
   "source": [
    "print(template)\n",
    "t1=time()\n",
    "labels = pd.read_csv(DATA_DIR + 'labels/{}.csv'.format(template))\n",
    "labels['event_timestamp'] = pd.to_datetime(labels['event_timestamp'])\n",
    "\n",
    "revisions = read_revisions(DATA_DIR + 'page_history/page_history-{}-meta-info.json'.format(template))\n",
    "pages = revisions.groupby('page_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5174"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing : 4286\n",
      "processing : 4287\n",
      "processing : 4288\n",
      "processing : 4289\n",
      "processing : 4290\n",
      "processing : 4291\n",
      "processing : 4292\n",
      "processing : 4293\n",
      "processing : 4294\n",
      "processing : 4295\n",
      "processing : 4296\n",
      "processing : 4297\n",
      "processing : 4298\n"
     ]
    }
   ],
   "source": [
    "i=len(features)\n",
    "restart = len(features)\n",
    "\n",
    "for tag_date, page_id in labels.values[restart:]:\n",
    "    print('processing :',i)\n",
    "    i+=1\n",
    "\n",
    "    page_revisions = pages.get_group(page_id)\n",
    "\n",
    "    # I think this can be done in a better way\n",
    "    tag_users = page_revisions[page_revisions.event_timestamp <= tag_date]\\\n",
    "                                .event_user_id.dropna().unique().tolist() \n",
    "    tag_user_histories = user_histories.filter(col('event_timestamp')<=tag_date)\\\n",
    "                                       .filter(col('event_user_id').isin(tag_users))\\\n",
    "                                       .withColumn('days_since_registration',f.datediff(f.lit(tag_date),col('event_user_registration_timestamp')))\n",
    "    #tag_user_histories = calculate_concentration_ratios(tag_user_histories)\\\n",
    "\n",
    "    editor_features = get_editor_features(tag_user_histories).toPandas().to_dict('records')\n",
    "\n",
    "    start_date = tag_date - relativedelta(years=1)\n",
    "    user_histories_1year = tag_user_histories.filter(col(\"event_timestamp\").between(start_date,tag_date))\n",
    "    editor_interactions = get_user_interactions(user_histories_1year)    \n",
    "\n",
    "    directed_features, undirected_features = calculate_collaboration_features(editor_interactions)\n",
    "    collaboration = {'directed':directed_features.toPandas().to_dict('records'), \n",
    "                     'undirected':undirected_features.toPandas().to_dict('records')}\n",
    "\n",
    "    features.append({'date':str(tag_date),'page_id':page_id,'editor':editor_features,\n",
    "                     'collaboration':collaboration})\n",
    "        \n",
    "with open(HOME +'features/editors'+template+'_v2.json','w') as file:\n",
    "    json.dump(features,file,default=np_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4286"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(HOME +'features/editors'+template+'_v2.json','w') as file:\n",
    "    json.dump(features,file,default=np_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark - YARN",
   "language": "python",
   "name": "spark_yarn_pyspark"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
