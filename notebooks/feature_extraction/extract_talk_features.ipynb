{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOME = '/srv/home/christinedk/wp_internship/'\n",
    "DATA_DIR = HOME + 'data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from dateutil import parser\n",
    "import datetime\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pylab as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sys\n",
    "sys.path.append('/srv/home/christinedk/wp_internship/collaboration/')\n",
    "from features.article_history import *\n",
    "from features.talk_history import *\n",
    "from utils import read_revisions, np_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_history-advert-meta-info.json         page_history-peacock-meta-info.json\r\n",
      "page_history-autobiography-meta-info.json  page_history-weasel-meta-info.json\r\n",
      "page_history-fanpov-meta-info.json\r\n"
     ]
    }
   ],
   "source": [
    "ls /srv/home/christinedk/wp_internship/data/page_history/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get subset from Talk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for template in ['autobiography','weasel','advert','fanpov','peacock']:\n",
    "    print(template)\n",
    "\n",
    "    article_talk_mappings = pd.read_csv(DATA_DIR+'article_talk_mappings/{}.csv'.format(template),\n",
    "                                       usecols=['talk_page_id','article_page_id'])\n",
    "    labels = pd.read_csv(DATA_DIR+'labels/{}.csv'.format(template),parse_dates=['event_timestamp'])\n",
    "    labels = labels.join(article_talk_mappings.set_index('article_page_id')[['talk_page_id']],on='page_id').dropna()\n",
    "    print('number of labels: ',len(labels))\n",
    "    \n",
    "    page_labels = labels.groupby('talk_page_id')\n",
    "    pages = labels.talk_page_id.unique()\n",
    "    \n",
    "    talk = {}\n",
    "    with open(DATA_DIR + 'talk_history/talk-text-{}-meta-info.json'.format(template),'rb') as f:\n",
    "        for line in f:\n",
    "            snapshot=json.loads(line)\n",
    "            page_id = snapshot['page_id']\n",
    "            if page_id not in pages:\n",
    "                continue\n",
    "\n",
    "            snapshot_date = parser.parse(snapshot['revision_timestamp']).replace(tzinfo=None)\n",
    "            page_lable_dates = page_labels.get_group(snapshot['page_id'])['event_timestamp']\n",
    "            date_diffs = (snapshot_date - page_lable_dates).dt.days\n",
    "            min_ind = date_diffs.idxmin()\n",
    "            min_diff = date_diffs[min_ind]\n",
    "            if 0 <= min_diff < talk.get((page_id,page_lable_dates[min_ind])[0],365):\n",
    "                talk[page_id,page_lable_dates[min_ind]]=(min_diff,snapshot)\n",
    "\n",
    "    print(len(talk))\n",
    "    talk_dump = [{'talk_page_id':key[0],'event_timestamp':str(key[1]),**value[1]} for key, value in talk.items()]\n",
    "    with open(DATA_DIR+'talk_history/talk-subset-{}.json'.format(template),'w') as f:\n",
    "        json.dump(talk_dump, f, default=np_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Talk features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "autobiography\n",
      "reading data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4224 [00:00<?, ?it/s]/srv/home/christinedk/wp_internship/collaboration/features/talk_history.py:87: FutureWarning: The User class is deprecated and will be removed in a future release. Use the Speaker class instead.\n",
      "100%|██████████| 4224/4224 [02:34<00:00, 27.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fanpov\n",
      "reading data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/721 [00:00<?, ?it/s]/srv/home/christinedk/wp_internship/collaboration/features/talk_history.py:87: FutureWarning: The User class is deprecated and will be removed in a future release. Use the Speaker class instead.\n",
      "100%|██████████| 721/721 [00:34<00:00, 21.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weasel\n",
      "reading data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1322 [00:00<?, ?it/s]/srv/home/christinedk/wp_internship/collaboration/features/talk_history.py:87: FutureWarning: The User class is deprecated and will be removed in a future release. Use the Speaker class instead.\n",
      "100%|██████████| 1322/1322 [03:31<00:00,  6.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "autobiography\n",
      "reading data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4224 [00:00<?, ?it/s]/srv/home/christinedk/wp_internship/collaboration/features/talk_history.py:87: FutureWarning: The User class is deprecated and will be removed in a future release. Use the Speaker class instead.\n",
      "100%|██████████| 4224/4224 [02:31<00:00, 27.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "advert\n",
      "reading data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7570 [00:00<?, ?it/s]/srv/home/christinedk/wp_internship/collaboration/features/talk_history.py:87: FutureWarning: The User class is deprecated and will be removed in a future release. Use the Speaker class instead.\n",
      "100%|██████████| 7570/7570 [05:44<00:00, 21.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peacock\n",
      "reading data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5174 [00:00<?, ?it/s]/srv/home/christinedk/wp_internship/collaboration/features/talk_history.py:87: FutureWarning: The User class is deprecated and will be removed in a future release. Use the Speaker class instead.\n",
      "100%|██████████| 5174/5174 [04:01<00:00, 21.47it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21min 30s, sys: 1min 3s, total: 22min 33s\n",
      "Wall time: 22min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "conv_parser = ConvParser()\n",
    "feature_extractor = FeatureExtractor()\n",
    "\n",
    "for template in ['autobiography','fanpov','weasel','autobiography','advert','peacock']:\n",
    "    print(template)\n",
    "    \n",
    "    print('reading data')\n",
    "    # read and format things\n",
    "    page_revisions = read_revisions(DATA_DIR+'page_history/page_history-{}-meta-info.json'.format(template))\n",
    "    talk_revisions = read_revisions(DATA_DIR+'talk_history/talk-activity-{}-meta-info.json'.format(template),\n",
    "                                    rename=True)\n",
    "    \n",
    "    talk_text = pd.read_json(DATA_DIR+'talk_history/talk-subset-{}.json'.format(template))\n",
    "    talk_text['event_timestamp'] = pd.to_datetime(talk_text['event_timestamp'])\n",
    "    talk_text = talk_text.set_index(['talk_page_id','event_timestamp'])[['revision_text','page_id']]\n",
    "\n",
    "    article_talk_mappings = pd.read_csv(DATA_DIR+'article_talk_mappings/{}.csv'.format(template),\n",
    "                                       usecols=['talk_page_id','article_page_id'])\n",
    "    labels = pd.read_csv(DATA_DIR+'labels/{}.csv'.format(template),parse_dates=['event_timestamp'])\n",
    "    labels = labels.join(article_talk_mappings.set_index('article_page_id')[['talk_page_id']],on='page_id').dropna()\n",
    "    \n",
    "    # prepare to extract by page\n",
    "    talk_pages = talk_revisions.groupby('page_id')\n",
    "    pages = page_revisions.groupby('page_id')\n",
    "    \n",
    "    features = []\n",
    "    counter = 0\n",
    "    for tag_date, page_id, talk_page_id in tqdm(labels.values):\n",
    "        lang_features = conv = talk_volume = {}\n",
    "        \n",
    "        tag_page_revisions = pages.get_group(page_id)\n",
    "        tag_page_revisions = tag_page_revisions[tag_page_revisions.event_timestamp.dt.date <= tag_date]\n",
    "\n",
    "        tag_talk_revisions = talk_pages.get_group(talk_page_id)\n",
    "        tag_talk_revisions = tag_talk_revisions[tag_talk_revisions.event_timestamp.dt.date <= tag_date]\n",
    "        \n",
    "        # talk page; volume\n",
    "        if len(tag_talk_revisions) > 0:\n",
    "            tag_page_revisions = calculate_page_metrics(tag_talk_revisions)\n",
    "            talk_features = get_talk_features(tag_talk_revisions)\n",
    "            talk_features['page_talk_ratio'] = len(tag_page_revisions)/len(tag_talk_revisions)\n",
    "\n",
    "        # talk page; language\n",
    "        if (talk_page_id,tag_date) in talk_text.index:\n",
    "            talk_latest = talk_text.loc[(talk_page_id,tag_date)].values\n",
    "            conv = conv_parser.format_conv(*talk_latest)\n",
    "            lang_features = feature_extractor.get_language_features(conv)\n",
    "\n",
    "        features.append({'page':page_id,'date':str(tag_date),\n",
    "                         'conversation':conv,\n",
    "                        'talk_volume':talk_features,\n",
    "                        'talk_language':lang_features})\n",
    "        \n",
    "    with open(HOME +'features/talk_'+template+'.json','w') as f:\n",
    "        json.dump(features,f,default=np_encoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preceding unsigned comment\n",
    "# heading \"section\"\n",
    "# section of current conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
