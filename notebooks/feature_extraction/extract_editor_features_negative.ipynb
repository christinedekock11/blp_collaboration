{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "HOME = '/srv/home/christinedk/wp_internship/'\n",
    "DATA_DIR = HOME + 'data/'\n",
    "sys.path.append(HOME + 'collaboration/')\n",
    "from config import TEMPLATES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "from time import time\n",
    "from math import log2\n",
    "import json\n",
    "\n",
    "import dateutil \n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import ArrayType, StringType, IntegerType, DoubleType, FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def talk_article_ratio(page_namespace_list):\n",
    "    page_namespace_list = np.array(page_namespace_list)\n",
    "    talk = np.sum(page_namespace_list==1) + 1\n",
    "    article = np.sum(page_namespace_list==0) + 1\n",
    "    return float(article/talk)\n",
    "udf_page_talk_ratio = udf(talk_article_ratio,FloatType())\n",
    "\n",
    "def entropy(p):\n",
    "    return -sum([p[i] * log2(p[i]) for i in range(len(p))])\n",
    "\n",
    "def contribution_fracs(page_ids):\n",
    "    counts = np.unique(page_ids,return_counts=True)[1]\n",
    "    cf = counts/sum(counts)\n",
    "    entropy_cf = float(entropy(cf))\n",
    "    return entropy_cf\n",
    "udf_contribution_frac = udf(contribution_fracs,FloatType())\n",
    "\n",
    "def read_revisions(filename, rename=False):\n",
    "    revisions = pd.read_json(filename,lines=True)\n",
    "    if rename:\n",
    "        revisions = revisions.rename(columns = {'revision_timestamp':'event_timestamp','user_id':'event_user_id'})\n",
    "    revisions['event_timestamp'] = pd.to_datetime(revisions['event_timestamp'])\n",
    "    revisions = revisions.sort_values(by='event_timestamp', ascending=True)\n",
    "    return revisions\n",
    "\n",
    "def np_encoder(object):\n",
    "    if isinstance(object, np.generic):\n",
    "        return object.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_editor_features(tag_user_histories):\n",
    "    \n",
    "    tag_features = tag_user_histories.groupby('event_user_id')\\\n",
    "         .agg(f.last('num_groups').alias('num_groups'),\n",
    "           f.countDistinct('page_id').alias('num_articles'),\n",
    "           f.count('revision_id').alias('num_edits'),\n",
    "           f.last('num_blocks_historical').alias('num_past_blocks'),\n",
    "           f.last('num_curr_blocks').alias('num_curr_blocks'),\n",
    "           f.sum(col(\"is_revert_bool\")).alias('num_reverts_by_others'),\n",
    "           f.sum(col('is_reverted_bool')).alias('num_reverts_of_others'),\n",
    "           f.last('days_since_registration').alias('time_since_registration'),\n",
    "           udf_page_talk_ratio(f.collect_list('page_namespace')).alias('talk_article_ratio'),\n",
    "           udf_contribution_frac(f.collect_list('page_id')).alias('contribution_frac_entropy') \n",
    "          )\n",
    "    \n",
    "    return tag_features\n",
    "\n",
    "def get_user_interactions(user_histories_1year):\n",
    "    user_page_revisions = user_histories_1year.select(col('page_id'),col('event_user_id'),\n",
    "                                                      col('revision_id'),col('page_namespace'))\\\n",
    "                                .groupBy(\"page_id\",\"event_user_id\").agg(\n",
    "                                f.count(\"revision_id\").alias(\"revisions_count\"),\n",
    "                                f.first(\"page_namespace\").alias('page_namespace'))\n",
    "                                #f.first(\"concentration_ratio\").alias('concentration_ratio'))\n",
    "\n",
    "    self_join_df = user_page_revisions.toDF(*[c + '_r' for c in user_page_revisions.columns])\n",
    "    editor_interactions = user_page_revisions.join(self_join_df,[user_page_revisions.page_id == self_join_df.page_id_r,\n",
    "                           user_page_revisions.event_user_id != self_join_df.event_user_id_r]).drop('page_id_r')\n",
    "    \n",
    "    return editor_interactions\n",
    "\n",
    "def calculate_concentration_ratios(user_histories):\n",
    "    concentration_ratio = user_histories.groupby('page_id').agg(\n",
    "                                    f.countDistinct('event_user_id').alias('num_editors'),\n",
    "                                    f.count('event_user_id').alias('num_revisions'))\\\n",
    "                                    .withColumn('concentration_ratio',col('num_editors')/col('num_revisions'))\n",
    "\n",
    "    user_histories = user_histories.join(concentration_ratio,on='page_id')\n",
    "    return user_histories\n",
    "\n",
    "def get_directed_features(paired_interactions):\n",
    "    user_article_edits = paired_interactions.groupby('event_user_id')\\\n",
    "                             .agg(f.sum('num_common_articles').alias('editor_pages_total'),\n",
    "                             f.sum('num_revisions_articles').alias('editor_revisions_total'))\n",
    "\n",
    "    directed = paired_interactions\\\n",
    "        .join(user_article_edits.select('event_user_id','editor_pages_total','editor_revisions_total'), on=\"event_user_id\")\\\n",
    "        .withColumn(\"coedit_ratio\", (col(\"num_common_articles\") / col(\"editor_pages_total\")))\\\n",
    "        .withColumn('coedit_revisions_ratio', (col('num_revisions_articles') / col('editor_revisions_total')))\\\n",
    "        .select('event_user_id','event_user_id_r','coedit_ratio', 'coedit_revisions_ratio')  \n",
    "    \n",
    "    return directed\n",
    "\n",
    "\n",
    "def get_undirected_features(paired_interactions, paired_interactions_articles):\n",
    "    features_all = paired_interactions.withColumn('pair',f.array_sort(f.array(col('event_user_id'),col('event_user_id_r'))))\\\n",
    "                            .drop_duplicates(subset=['pair'])\\\n",
    "                            .select('pair','num_common_pages')\n",
    "    features_articles = paired_interactions_articles.withColumn('pair',f.array_sort(f.array(col('event_user_id'),col('event_user_id_r'))))\\\n",
    "                                    .drop_duplicates(subset=['pair'])\\\n",
    "                                    .select('pair','num_common_articles')#,'mean_concentration_ratio')\n",
    "\n",
    "    undirected = features_all.join(features_articles,on='pair')\n",
    "    return undirected\n",
    "\n",
    "def calculate_collaboration_features(editor_interactions):\n",
    "    #editor_interactions = calculate_concentration_ratios(editor_interactions)\n",
    "    \n",
    "    paired_interactions_articles = editor_interactions.filter(col('page_namespace')==0)\\\n",
    "                            .groupby('event_user_id','event_user_id_r')\\\n",
    "                            .agg(f.count(\"page_id\").alias('num_common_articles'),\n",
    "                                 f.sum('revisions_count').alias('num_revisions_articles'))\\\n",
    "                            .cache()\n",
    "                            #f.mean('concentration_ratio').alias('mean_concentration_ratio')\n",
    "                            #.filter(col('num_common_articles')>=5)\n",
    "    \n",
    "    paired_interactions_all = editor_interactions.groupby('event_user_id','event_user_id_r')\\\n",
    "                                             .agg(f.count(\"page_id\").alias('num_common_pages'))\n",
    "    \n",
    "    directed_features = get_directed_features(paired_interactions_articles)\n",
    "    undirected_features = get_undirected_features(paired_interactions_all, paired_interactions_articles)\n",
    "                                                  \n",
    "    return directed_features,undirected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "user_histories = spark.read.parquet('editors/user_histories.parquet')\n",
    "user_histories = user_histories.withColumn('event_timestamp',\n",
    "                                           f.to_timestamp(col('event_timestamp')))\\\n",
    "                                .withColumn('event_user_registration_timestamp',\n",
    "                                           f.to_timestamp(col('event_user_registration_timestamp')))\\\n",
    "                                .withColumn('is_revert_bool',col(\"revision_is_identity_revert\").cast(\"long\"))\\\n",
    "                                .withColumn('is_reverted_bool',col(\"revision_is_identity_reverted\").cast(\"long\"))\n",
    "\n",
    "user_histories = user_histories.orderBy(col('event_timestamp'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34marticle_talk_mappings\u001b[0m/  \u001b[01;34mdata_export_1402\u001b[0m/  namespaces.csv    \u001b[01;34mtalk_history\u001b[0m/\r\n",
      "\u001b[01;34mblp\u001b[0m/                    \u001b[01;34meditor_history\u001b[0m/    \u001b[01;34mnegative_labels\u001b[0m/  \u001b[01;34mtalk_split\u001b[0m/\r\n",
      "blp_articles.csv        \u001b[01;34meditor_split\u001b[0m/      \u001b[01;34mpage_history\u001b[0m/\r\n",
      "categories.csv          \u001b[01;34mlabels\u001b[0m/            \u001b[01;34mtag_events\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "ls /srv/home/christinedk/wp_internship/data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weasel\n",
      "total:  6610\n",
      "processing : 0\n",
      "processing : 1\n",
      "processing : 2\n",
      "processing : 3\n",
      "processing : 4\n",
      "processing : 5\n",
      "processing : 6\n",
      "processing : 7\n",
      "processing : 8\n",
      "processing : 9\n",
      "processing : 10\n",
      "processing : 11\n",
      "processing : 12\n",
      "processing : 13\n",
      "processing : 14\n",
      "processing : 15\n",
      "processing : 16\n",
      "processing : 17\n",
      "processing : 18\n",
      "processing : 19\n",
      "processing : 20\n",
      "processing : 21\n",
      "processing : 22\n",
      "processing : 23\n",
      "processing : 24\n",
      "processing : 25\n",
      "processing : 26\n",
      "processing : 27\n",
      "processing : 28\n",
      "processing : 29\n",
      "processing : 30\n",
      "processing : 31\n",
      "processing : 32\n",
      "processing : 33\n",
      "processing : 34\n",
      "processing : 35\n",
      "processing : 36\n",
      "processing : 37\n",
      "processing : 38\n",
      "processing : 39\n",
      "processing : 40\n",
      "processing : 41\n",
      "processing : 42\n",
      "processing : 43\n",
      "processing : 44\n",
      "processing : 45\n",
      "processing : 46\n",
      "processing : 47\n",
      "processing : 48\n",
      "processing : 49\n",
      "processing : 50\n",
      "processing : 51\n",
      "processing : 52\n",
      "processing : 53\n",
      "processing : 54\n",
      "processing : 55\n",
      "processing : 56\n",
      "processing : 57\n",
      "processing : 58\n",
      "processing : 59\n",
      "processing : 60\n",
      "processing : 61\n",
      "processing : 62\n",
      "processing : 63\n",
      "processing : 64\n",
      "processing : 65\n",
      "processing : 66\n",
      "processing : 67\n",
      "processing : 68\n",
      "processing : 69\n",
      "processing : 70\n",
      "processing : 71\n",
      "processing : 72\n",
      "processing : 73\n",
      "processing : 74\n",
      "processing : 75\n",
      "processing : 76\n",
      "processing : 77\n",
      "processing : 78\n",
      "processing : 79\n",
      "processing : 80\n",
      "processing : 81\n",
      "processing : 82\n",
      "processing : 83\n",
      "processing : 84\n",
      "processing : 85\n",
      "processing : 86\n",
      "processing : 87\n",
      "processing : 88\n",
      "processing : 89\n",
      "processing : 90\n",
      "processing : 91\n",
      "processing : 92\n",
      "processing : 93\n",
      "processing : 94\n",
      "processing : 95\n",
      "processing : 96\n",
      "processing : 97\n",
      "processing : 98\n",
      "processing : 99\n",
      "processing : 100\n",
      "processing : 101\n",
      "processing : 102\n",
      "processing : 103\n",
      "processing : 104\n",
      "processing : 105\n",
      "processing : 106\n",
      "processing : 107\n",
      "processing : 108\n",
      "processing : 109\n",
      "processing : 110\n",
      "processing : 111\n",
      "processing : 112\n",
      "processing : 113\n",
      "processing : 114\n",
      "processing : 115\n",
      "processing : 116\n",
      "processing : 117\n",
      "processing : 118\n",
      "processing : 119\n",
      "processing : 120\n",
      "processing : 121\n",
      "processing : 122\n",
      "processing : 123\n",
      "processing : 124\n",
      "processing : 125\n",
      "processing : 126\n",
      "processing : 127\n",
      "processing : 128\n",
      "processing : 129\n",
      "processing : 130\n",
      "processing : 131\n",
      "processing : 132\n",
      "processing : 133\n",
      "processing : 134\n",
      "processing : 135\n",
      "processing : 136\n",
      "processing : 137\n",
      "processing : 138\n",
      "processing : 139\n",
      "processing : 140\n",
      "processing : 141\n",
      "processing : 142\n",
      "processing : 143\n",
      "processing : 144\n",
      "processing : 145\n",
      "processing : 146\n",
      "processing : 147\n",
      "processing : 148\n",
      "processing : 149\n",
      "processing : 150\n",
      "processing : 151\n",
      "processing : 152\n",
      "processing : 153\n",
      "processing : 154\n",
      "processing : 155\n",
      "processing : 156\n",
      "processing : 157\n",
      "processing : 158\n",
      "processing : 159\n",
      "processing : 160\n",
      "processing : 161\n",
      "processing : 162\n",
      "processing : 163\n",
      "processing : 164\n",
      "processing : 165\n",
      "processing : 166\n",
      "processing : 167\n",
      "processing : 168\n",
      "processing : 169\n",
      "processing : 170\n",
      "processing : 171\n",
      "processing : 172\n",
      "processing : 173\n",
      "processing : 174\n",
      "processing : 175\n",
      "processing : 176\n",
      "processing : 177\n",
      "processing : 178\n",
      "processing : 179\n",
      "processing : 180\n",
      "processing : 181\n",
      "processing : 182\n",
      "processing : 183\n",
      "processing : 184\n",
      "processing : 185\n",
      "processing : 186\n",
      "processing : 187\n",
      "processing : 188\n",
      "processing : 189\n",
      "processing : 190\n",
      "processing : 191\n",
      "processing : 192\n",
      "processing : 193\n",
      "processing : 194\n",
      "processing : 195\n",
      "processing : 196\n",
      "processing : 197\n",
      "processing : 198\n",
      "processing : 199\n",
      "processing : 200\n",
      "processing : 201\n",
      "processing : 202\n",
      "processing : 203\n",
      "processing : 204\n",
      "processing : 205\n",
      "processing : 206\n",
      "processing : 207\n",
      "processing : 208\n",
      "processing : 209\n",
      "processing : 210\n",
      "processing : 211\n",
      "processing : 212\n",
      "processing : 213\n",
      "processing : 214\n",
      "processing : 215\n",
      "processing : 216\n",
      "processing : 217\n",
      "processing : 218\n",
      "processing : 219\n",
      "processing : 220\n",
      "processing : 221\n",
      "processing : 222\n",
      "processing : 223\n",
      "processing : 224\n",
      "processing : 225\n",
      "processing : 226\n",
      "processing : 227\n",
      "processing : 228\n",
      "processing : 229\n",
      "processing : 230\n",
      "processing : 231\n",
      "processing : 232\n",
      "processing : 233\n",
      "processing : 234\n",
      "processing : 235\n",
      "processing : 236\n",
      "processing : 237\n",
      "processing : 238\n",
      "processing : 239\n",
      "processing : 240\n",
      "processing : 241\n",
      "processing : 242\n"
     ]
    }
   ],
   "source": [
    "for template in ['weasel','fanpov', 'autobiography','peacock','advert']:\n",
    "    print(template)\n",
    "    t1=time()\n",
    "    labels = pd.read_csv(DATA_DIR + 'negative_labels/{}.csv'.format(template))\n",
    "    labels['event_timestamp'] = pd.to_datetime(labels['event_timestamp'])\n",
    "\n",
    "    revisions = read_revisions(DATA_DIR + 'page_history/page_history-{}-meta-info.json'.format(template))\n",
    "\n",
    "    pages = revisions.groupby('page_id')\n",
    "\n",
    "    print('total: ',len(labels))\n",
    "    i=0\n",
    "    for tag_date, page_id in labels.values:\n",
    "        print('processing :',i)\n",
    "        i+=1\n",
    "\n",
    "        page_revisions = pages.get_group(page_id)\n",
    "        \n",
    "        # I think this can be done in a better way\n",
    "        tag_users = page_revisions[page_revisions.event_timestamp <= tag_date]\\\n",
    "                                    .event_user_id.dropna().unique().tolist()\n",
    "        tag_user_histories = user_histories.filter(col('event_timestamp')<=tag_date)\\\n",
    "                                           .filter(col('event_user_id').isin(tag_users))\\\n",
    "                                           .withColumn('days_since_registration',f.datediff(f.lit(tag_date),col('event_user_registration_timestamp')))\n",
    "        #tag_user_histores = calculate_concentration_ratios(tag_user_histories)\\\n",
    "\n",
    "\n",
    "        editor_features = get_editor_features(tag_user_histories).toPandas().to_dict('records')\n",
    "\n",
    "        start_date = tag_date - relativedelta(years=1)\n",
    "        user_histories_1year = tag_user_histories.filter(col(\"event_timestamp\").between(start_date,tag_date))\n",
    "        editor_interactions = get_user_interactions(user_histories_1year)    \n",
    "\n",
    "        directed_features, undirected_features = calculate_collaboration_features(editor_interactions)\n",
    "        collaboration = {'directed':directed_features.toPandas().to_dict('records'), \n",
    "                         'undirected':undirected_features.toPandas().to_dict('records')}\n",
    "\n",
    "        features= {'date':str(tag_date),'page_id':page_id,'editor':editor_features,\n",
    "                         'collaboration':collaboration}\n",
    "        \n",
    "        with open(HOME +'negative_features/editors'+template+'.json','a+') as file:\n",
    "            json.dump(features,file,default=np_encoder)\n",
    "    \n",
    "    print('time: ',int(time()-t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 899M\r\n",
      "-rw-r--r-- 1 christinedk wikidev 236M Mar 29 23:46 activity_peacock.json\r\n",
      "-rw-r--r-- 1 christinedk wikidev 307M Mar 29 23:38 activity_advert.json\r\n",
      "-rw-r--r-- 1 christinedk wikidev 114M Mar 29 23:25 activity_autobiography.json\r\n",
      "-rw-r--r-- 1 christinedk wikidev 172M Mar 29 23:18 activity_weasel.json\r\n",
      "-rw-r--r-- 1 christinedk wikidev  48M Mar 29 23:15 activity_fanpov.json\r\n",
      "-rw-r--r-- 1 christinedk wikidev  11M Mar 29 21:39 talk_autobiography.json\r\n",
      "-rw-r--r-- 1 christinedk wikidev  11M Mar 29 21:34 talk_weasel.json\r\n",
      "-rw-r--r-- 1 christinedk wikidev 2.6M Mar 29 21:29 talk_fanpov.json\r\n"
     ]
    }
   ],
   "source": [
    "ls -lth /srv/home/christinedk/wp_internship/negative_features/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(HOME +'features/editors'+template+'.json','w') as file:\n",
    "    json.dump(features,file,default=np_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1160"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "template='weasel'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(HOME +'features/editors'+template+'.json','rb') as file:\n",
    "    features = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weasel\n"
     ]
    }
   ],
   "source": [
    "print(template)\n",
    "t1=time()\n",
    "labels = pd.read_csv(DATA_DIR + 'labels/{}.csv'.format(template))\n",
    "labels['event_timestamp'] = pd.to_datetime(labels['event_timestamp'])\n",
    "\n",
    "revisions = read_revisions(DATA_DIR + 'page_history/page_history-{}-meta-info.json'.format(template))\n",
    "pages = revisions.groupby('page_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing : 1161\n",
      "processing : 1162\n",
      "processing : 1163\n",
      "processing : 1164\n",
      "processing : 1165\n",
      "processing : 1166\n",
      "processing : 1167\n",
      "processing : 1168\n",
      "processing : 1169\n",
      "processing : 1170\n",
      "processing : 1171\n",
      "processing : 1172\n",
      "processing : 1173\n",
      "processing : 1174\n",
      "processing : 1175\n",
      "processing : 1176\n",
      "processing : 1177\n",
      "processing : 1178\n",
      "processing : 1179\n",
      "processing : 1180\n",
      "processing : 1181\n",
      "processing : 1182\n",
      "processing : 1183\n",
      "processing : 1184\n",
      "processing : 1185\n",
      "processing : 1186\n",
      "processing : 1187\n",
      "processing : 1188\n",
      "processing : 1189\n",
      "processing : 1190\n",
      "processing : 1191\n",
      "processing : 1192\n",
      "processing : 1193\n",
      "processing : 1194\n",
      "processing : 1195\n",
      "processing : 1196\n",
      "processing : 1197\n",
      "processing : 1198\n",
      "processing : 1199\n",
      "processing : 1200\n",
      "processing : 1201\n",
      "processing : 1202\n",
      "processing : 1203\n",
      "processing : 1204\n",
      "processing : 1205\n",
      "processing : 1206\n",
      "processing : 1207\n",
      "processing : 1208\n",
      "processing : 1209\n",
      "processing : 1210\n",
      "processing : 1211\n",
      "processing : 1212\n",
      "processing : 1213\n",
      "processing : 1214\n",
      "processing : 1215\n",
      "processing : 1216\n",
      "processing : 1217\n",
      "processing : 1218\n",
      "processing : 1219\n",
      "processing : 1220\n",
      "processing : 1221\n",
      "processing : 1222\n",
      "processing : 1223\n",
      "processing : 1224\n",
      "processing : 1225\n",
      "processing : 1226\n",
      "processing : 1227\n",
      "processing : 1228\n",
      "processing : 1229\n",
      "processing : 1230\n",
      "processing : 1231\n",
      "processing : 1232\n",
      "processing : 1233\n",
      "processing : 1234\n",
      "processing : 1235\n",
      "processing : 1236\n",
      "processing : 1237\n",
      "processing : 1238\n",
      "processing : 1239\n",
      "processing : 1240\n",
      "processing : 1241\n",
      "processing : 1242\n",
      "processing : 1243\n",
      "processing : 1244\n",
      "processing : 1245\n",
      "processing : 1246\n",
      "processing : 1247\n",
      "processing : 1248\n",
      "processing : 1249\n",
      "processing : 1250\n",
      "processing : 1251\n",
      "processing : 1252\n",
      "processing : 1253\n",
      "processing : 1254\n",
      "processing : 1255\n",
      "processing : 1256\n",
      "processing : 1257\n",
      "processing : 1258\n",
      "processing : 1259\n",
      "processing : 1260\n",
      "processing : 1261\n",
      "processing : 1262\n",
      "processing : 1263\n",
      "processing : 1264\n",
      "processing : 1265\n",
      "processing : 1266\n",
      "processing : 1267\n",
      "processing : 1268\n",
      "processing : 1269\n",
      "processing : 1270\n",
      "processing : 1271\n",
      "processing : 1272\n",
      "processing : 1273\n",
      "processing : 1274\n",
      "processing : 1275\n",
      "processing : 1276\n",
      "processing : 1277\n",
      "processing : 1278\n",
      "processing : 1279\n",
      "processing : 1280\n",
      "processing : 1281\n",
      "processing : 1282\n",
      "processing : 1283\n",
      "processing : 1284\n",
      "processing : 1285\n",
      "processing : 1286\n",
      "processing : 1287\n",
      "processing : 1288\n",
      "processing : 1289\n",
      "processing : 1290\n",
      "processing : 1291\n",
      "processing : 1292\n",
      "processing : 1293\n",
      "processing : 1294\n",
      "processing : 1295\n",
      "processing : 1296\n",
      "processing : 1297\n",
      "processing : 1298\n",
      "processing : 1299\n",
      "processing : 1300\n",
      "processing : 1301\n",
      "processing : 1302\n",
      "processing : 1303\n",
      "processing : 1304\n",
      "processing : 1305\n",
      "processing : 1306\n",
      "processing : 1307\n",
      "processing : 1308\n",
      "processing : 1309\n",
      "processing : 1310\n",
      "processing : 1311\n",
      "processing : 1312\n",
      "processing : 1313\n",
      "processing : 1314\n",
      "processing : 1315\n",
      "processing : 1316\n",
      "processing : 1317\n",
      "processing : 1318\n",
      "processing : 1319\n",
      "processing : 1320\n",
      "processing : 1321\n",
      "processing : 1322\n"
     ]
    }
   ],
   "source": [
    "i=len(features)\n",
    "\n",
    "for tag_date, page_id in labels.values[1160:]:\n",
    "    print('processing :',i)\n",
    "    i+=1\n",
    "\n",
    "    page_revisions = pages.get_group(page_id)\n",
    "\n",
    "    # I think this can be done in a better way\n",
    "    tag_users = page_revisions[page_revisions.event_timestamp <= tag_date]\\\n",
    "                                .event_user_id.dropna().unique().tolist() \n",
    "    tag_user_histories = user_histories.filter(col('event_timestamp')<=tag_date)\\\n",
    "                                       .filter(col('event_user_id').isin(tag_users))\\\n",
    "                                       .withColumn('days_since_registration',f.datediff(f.lit(tag_date),col('event_user_registration_timestamp')))\n",
    "    #tag_user_histories = calculate_concentration_ratios(tag_user_histories)\\\n",
    "\n",
    "    editor_features = get_editor_features(tag_user_histories).toPandas().to_dict('records')\n",
    "\n",
    "    start_date = tag_date - relativedelta(years=1)\n",
    "    user_histories_1year = tag_user_histories.filter(col(\"event_timestamp\").between(start_date,tag_date))\n",
    "    editor_interactions = get_user_interactions(user_histories_1year)    \n",
    "\n",
    "    directed_features, undirected_features = calculate_collaboration_features(editor_interactions)\n",
    "    collaboration = {'directed':directed_features.toPandas().to_dict('records'), \n",
    "                     'undirected':undirected_features.toPandas().to_dict('records')}\n",
    "\n",
    "    features.append({'date':str(tag_date),'page_id':page_id,'editor':editor_features,\n",
    "                     'collaboration':collaboration})\n",
    "        \n",
    "with open(HOME +'features/editors'+template+'_v2.json','w') as file:\n",
    "    json.dump(features,file,default=np_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 231 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "251 266"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = 'fanpov'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total:  721\n",
      "processing : 0\n",
      "num records:  5051\n",
      "processing : 1\n",
      "num records:  5702\n",
      "processing : 2\n",
      "num records:  506\n",
      "processing : 3\n",
      "num records:  1615\n",
      "processing : 4\n",
      "num records:  1000\n",
      "processing : 5\n",
      "num records:  7677\n",
      "processing : 6\n",
      "num records:  7959\n",
      "processing : 7\n",
      "num records:  2010\n",
      "processing : 8\n",
      "num records:  1218\n",
      "processing : 9\n",
      "num records:  32\n",
      "CPU times: user 4.35 s, sys: 1.08 s, total: 5.43 s\n",
      "Wall time: 28min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "features = []\n",
    "pages = revisions.groupby('page_id')\n",
    "\n",
    "print('total: ',len(labels))\n",
    "i=0\n",
    "\n",
    "for tag_date, page_id in labels.values[:10]:\n",
    "    print('\\t processing :',i)\n",
    "    i+=1\n",
    "    \n",
    "    page_revisions = pages.get_group(page_id)    \n",
    "    tag_users = page_revisions[page_revisions.event_timestamp <= tag_date]\\\n",
    "                                .event_user_id.dropna().unique().tolist() # I think this can be done in a better way\n",
    "    tag_user_histories = user_histories.filter(col('event_timestamp')<tag_date)\\\n",
    "                                       .filter(col('event_user_id').isin(tag_users))\\\n",
    "                                       .withColumn('days_since_registration',f.datediff(f.lit(tag_date),col('event_user_registration_timestamp')))\n",
    "\n",
    "    editor_features = get_editor_features(tag_user_histories).toPandas().to_dict('records')\n",
    "\n",
    "    start_date = tag_date - relativedelta(years=1)\n",
    "    user_histories_1year = tag_user_histories.filter(col(\"event_timestamp\").between(start_date,tag_date))\n",
    "    editor_interactions = get_user_interactions(user_histories_1year)    \n",
    "\n",
    "    directed_features, undirected_features = calculate_collaboration_features(editor_interactions)\n",
    "    collaboration = {'directed':directed_features.toPandas().to_dict('records'), \n",
    "                     'undirected':undirected_features.toPandas().to_dict('records')}\n",
    "    \n",
    "    print('\\t\\t num records: ',len(collaboration['undirected']))\n",
    "\n",
    "    features.append({'date':tag_date,'page_id':page_id,'editor':editor_features,\n",
    "                     'collaboration':collaboration})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark - YARN",
   "language": "python",
   "name": "spark_yarn_pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
