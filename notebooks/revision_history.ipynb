{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/christinedk/wp_internship/collaboration/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time  import time\n",
    "from data_export import getTemplatesRegexRelaibility, getTemplatesRegex\n",
    "from pyspark.sql.functions import udf, col, explode, regexp_replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: `page_history': File exists\r\n"
     ]
    }
   ],
   "source": [
    "TEMPLATES = ['weasel','peacock','autobiography','advert','fanpov']\n",
    "outputHDFS = 'page_history'\n",
    "!hadoop fs -mkdir $outputHDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get all revisions with templates from WikiText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshot =\"2020-09\"\n",
    "wikidb = \"enwiki\"\n",
    "wikitext_history = spark.sql('''SELECT page_id,page_title,revision_id,revision_text,user_id\n",
    "    FROM wmf.mediawiki_wikitext_history \n",
    "    WHERE snapshot =\"{snapshot}\" and wiki_db =\"{wikidb}\"'''.format(wikidb=wikidb,snapshot=snapshot))\n",
    "\n",
    "## Apply getTemplatesRegexRelaibility over all wikitext history\n",
    "wikitext_history = wikitext_history.withColumn(\"templates\",getTemplatesRegexRelaibility(col('revision_text')))\n",
    "\n",
    "revisions_with_template = wikitext_history.select(wikitext_history.page_id,wikitext_history.page_title,wikitext_history.user_id,wikitext_history.revision_id,explode(wikitext_history.templates))\n",
    "revisions_with_template = revisions_with_template.withColumn('page_title', regexp_replace('page_title', ' ', '_'))\n",
    "\n",
    "## Persist outputs\n",
    "revisions_with_template.write.parquet(outputHDFS+'/templates.parquet',mode='overwrite')\n",
    "\n",
    "## Read back revisions with template\n",
    "revisions_with_template = spark.read.parquet(outputHDFS+'/templates.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get full edit history of pages and editors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Select subset of mediawiki history containing all page titles with revisions\n",
    "\n",
    "pages_templates_subset = revisions_with_template.select('page_title').distinct()\n",
    "pages_templates_subset.createOrReplaceTempView('pages_templates_subset')\n",
    "\n",
    "mediawiki_history_subset =  spark.sql('''\n",
    "        SELECT w.event_timestamp, w.page_title,w.page_id,w.page_namespace, \n",
    "        w.revision_id, w.revision_is_identity_reverted, \n",
    "        w.revision_minor_edit, w.revision_text_bytes, \n",
    "        w.revision_first_identity_reverting_revision_id, w.revision_seconds_to_identity_revert,\n",
    "        w.event_user_id,w.event_user_registration_timestamp, \n",
    "        w.event_user_is_anonymous,w.event_user_revision_count,\n",
    "\n",
    "        w.event_comment\n",
    "        FROM wmf.mediawiki_history w\n",
    "        WHERE w.snapshot =\"2020-09\" and w.wiki_db =\"enwiki\" AND  \n",
    "      w.event_entity = 'revision' AND w.page_title IN (\n",
    "                    SELECT  page_title FROM pages_templates_subset)                   \n",
    "        ''')\n",
    "mediawiki_history_subset.cache()\n",
    "mediawiki_history_subset.createOrReplaceTempView('mediawiki_history_subset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weasel\n",
      "read table, done 0.07624244689941406\n",
      "save table, done 261.32771944999695\n",
      "21/02/23 16:41:27 INFO compress.CodecPool: Got brand-new decompressor [.snappy]\n",
      "----- 19.260557174682617\n",
      "peacock\n",
      "read table, done 0.0747826099395752\n",
      "save table, done 62.637932538986206\n",
      "21/02/23 16:42:49 INFO compress.CodecPool: Got brand-new decompressor [.snappy]\n",
      "----- 10.20436406135559\n",
      "autobiography\n",
      "read table, done 0.06866979598999023\n",
      "save table, done 42.912111043930054\n",
      "21/02/23 16:43:43 INFO compress.CodecPool: Got brand-new decompressor [.snappy]\n",
      "----- 3.1491854190826416\n",
      "advert\n",
      "read table, done 0.06957197189331055\n",
      "save table, done 84.30075573921204\n",
      "21/02/23 16:45:10 INFO compress.CodecPool: Got brand-new decompressor [.snappy]\n",
      "----- 27.113717317581177\n",
      "fanpov\n",
      "read table, done 0.07831907272338867\n",
      "save table, done 35.95526933670044\n",
      "21/02/23 16:46:13 INFO compress.CodecPool: Got brand-new decompressor [.snappy]\n",
      "----- 3.868117570877075\n"
     ]
    }
   ],
   "source": [
    "## Get full histories of these pages *and* all editors who revised them\n",
    "\n",
    "for template in templates:\n",
    "    try:\n",
    "        t1 = time()\n",
    "        print(template)\n",
    "        df = revisions_with_template.where(revisions_with_template['col']==template) # \n",
    "        df.cache()\n",
    "        t2 = time()\n",
    "        print('read table, done',t2-t1)\n",
    "        t1 = time()\n",
    "        page_ids = df.select('page_title').distinct()\n",
    "        page_ids.createOrReplaceTempView('tmp_page_ids')\n",
    "        revision_ids = df.select('revision_id').distinct()\n",
    "        revision_ids.createOrReplaceTempView('tmp_revision_ids')\n",
    "        reverts = spark.sql('''\n",
    "        SELECT w.event_timestamp, w.page_title,w.page_id, w.page_namespace,\n",
    "        w.revision_id, w.revision_is_identity_reverted, \n",
    "        w.revision_minor_edit, w.revision_text_bytes, \n",
    "        w.revision_first_identity_reverting_revision_id, w.revision_seconds_to_identity_revert,\n",
    "        w.event_user_id,w.event_user_registration_timestamp, \n",
    "        w.event_user_is_anonymous,w.event_user_revision_count,\n",
    "        CASE WHEN r.revision_id IS NOT NULL  THEN 1 ELSE 0 END has_template,\n",
    "        w.event_comment\n",
    "        FROM mediawiki_history_subset w \n",
    "        LEFT OUTER JOIN tmp_revision_ids r \n",
    "        ON (w.revision_id = r.revision_id)\n",
    "        WHERE  w.page_title IN (SELECT page_title FROM tmp_page_ids) \n",
    "        ORDER BY page_title, w.revision_id\n",
    "        ''') \n",
    "        reverts.repartition(1).write.format('json').save(outputHDFS+'/'+template,mode='overwrite')\n",
    "        reverts.cache()\n",
    "        t2 = time()\n",
    "        print('save table, done',t2-t1)\n",
    "        t1 = time()   \n",
    "        templateout = template.replace(' ','_')\n",
    "        !hadoop fs -text \"$outputHDFS/$template/*\" > $outputHDFS-$template-meta-info.json\n",
    "        t2 = time()\n",
    "        print('-----',t2-t1)\n",
    "        \n",
    "        print('extracting user histories')\n",
    "        outputHDFS = 'user_history'\n",
    "        reverts.createOrReplaceTempView('full_page_history')\n",
    "        user_ids = full_page_history.select('event_user_id').distinct()\n",
    "        user_ids.createOrReplaceTempView('tmp_user_ids')\n",
    "        user_histories =  spark.sql('''\n",
    "        SELECT w.event_timestamp, w.page_title,w.page_id,w.page_namespace, \n",
    "        w.revision_id, w.revision_is_identity_reverted, \n",
    "        w.revision_minor_edit, w.revision_text_bytes, \n",
    "        w.revision_first_identity_reverting_revision_id, w.revision_seconds_to_identity_revert,\n",
    "        w.event_user_id,w.event_user_registration_timestamp, \n",
    "        w.event_user_is_anonymous,w.event_user_revision_count,\n",
    "        w.event_comment\n",
    "        FROM wmf.mediawiki_history w\n",
    "        WHERE w.snapshot =\"2020-09\" and w.wiki_db =\"enwiki\" AND  \n",
    "        w.event_entity = 'revision' \n",
    "        AND w.event_user_id IN (SELECT event_user_id FROM tmp_user_ids)                   \n",
    "        ''')\n",
    "        \n",
    "        user_histories.repartition(1).write.format('json').save(outputHDFS+'/'+template,mode='overwrite')\n",
    "        t2 = time()\n",
    "        print('save table, done',t2-t1)\n",
    "        t1 = time()   \n",
    "        templateout = template.replace(' ','_')\n",
    "        !hadoop fs -text \"$outputHDFS/$template/*\" > $outputHDFS-$template-meta-info.json\n",
    "        t2 = time()\n",
    "        print('-----',t2-t1)\n",
    "        \n",
    "        \n",
    "    except Exception as e:\n",
    "        print('error',e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 christinedk wikidev 725M Feb 23 11:05 tmp-advert-meta-info.csv.gz\r\n",
      "-rw-r--r-- 1 christinedk wikidev 7.0G Feb 23 11:55 tmp-advert-meta-info.json\r\n",
      "-rw-r--r-- 1 christinedk wikidev  39M Feb 23 11:00 tmp-autobiography-meta-info.csv.gz\r\n",
      "-rw-r--r-- 1 christinedk wikidev 371M Feb 23 11:53 tmp-autobiography-meta-info.json\r\n",
      "-rw-r--r-- 1 christinedk wikidev  64M Feb 23 11:08 tmp-fanpov-meta-info.csv.gz\r\n",
      "-rw-r--r-- 1 christinedk wikidev 687M Feb 23 11:55 tmp-fanpov-meta-info.json\r\n",
      "-rw-r--r-- 1 christinedk wikidev 349M Feb 23 10:59 tmp-peacock-meta-info.csv.gz\r\n",
      "-rw-r--r-- 1 christinedk wikidev 3.5G Feb 23 11:53 tmp-peacock-meta-info.json\r\n",
      "-rw-r--r-- 1 christinedk wikidev 731M Feb 23 10:56 tmp-weasel-meta-info.csv.gz\r\n"
     ]
    }
   ],
   "source": [
    "ls -lh tmp-*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark - YARN (large)",
   "language": "python",
   "name": "spark_yarn_pyspark_large"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
